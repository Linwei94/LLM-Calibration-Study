{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb2f0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "OPENAI_SYSTEM_MESSAGE_API = \"You are a helpful assistant.\"\n",
    "OPENAI_SYSTEM_MESSAGE_CHATGPT = (\n",
    "    \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "    + \"\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\"\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCompletionSampler():\n",
    "    \"\"\"\n",
    "    Sample from OpenAI's chat completion API\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        system_message: str | None = None,\n",
    "        temperature: float = 0.5,\n",
    "        max_tokens: int = 1024,\n",
    "        base_url=None,\n",
    "        api_key=None,\n",
    "        logprobs = False\n",
    "    ):\n",
    "        self.api_key_name = \"OPENAI_API_KEY\"\n",
    "        if base_url and any(provider in base_url for provider in [\"google\", \"databricks\", \"together\"]):\n",
    "            self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        else:\n",
    "            self.client = OpenAI()\n",
    "        # using api_key=os.environ.get(\"OPENAI_API_KEY\")  # please set your API_KEY\n",
    "        self.model = model\n",
    "        self.system_message = system_message\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.image_format = \"url\"\n",
    "        self.logprobs = logprobs\n",
    "        self.top_logprobs = None\n",
    "\n",
    "    def _handle_image(\n",
    "        self, image: str, encoding: str = \"base64\", format: str = \"png\", fovea: int = 768\n",
    "    ):\n",
    "        new_image = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/{format};{encoding},{image}\",\n",
    "            },\n",
    "        }\n",
    "        return new_image\n",
    "\n",
    "    def _handle_text(self, text: str):\n",
    "        return {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    def _pack_message(self, role: str, content: Any):\n",
    "        return {\"role\": str(role), \"content\": content}\n",
    "\n",
    "    def __call__(self, message_list) -> str:\n",
    "        if self.system_message:\n",
    "            message_list = [self._pack_message(\"system\", self.system_message)] + message_list\n",
    "        trial = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if self.logprobs:\n",
    "                    try:\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            model=self.model,\n",
    "                            messages=message_list,\n",
    "                            temperature=self.temperature,\n",
    "                            max_tokens=self.max_tokens,\n",
    "                            logprobs=self.logprobs,\n",
    "                            top_logprobs=10\n",
    "                        )\n",
    "                    except:\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            model=self.model,\n",
    "                            messages=message_list,\n",
    "                            temperature=self.temperature,\n",
    "                            max_tokens=self.max_tokens,\n",
    "                            logprobs=10\n",
    "                        )\n",
    "                    try:\n",
    "                        self.top_logprobs = [t.top_logprobs for t in response.choices[0].logprobs.content]\n",
    "                        return response.choices[0].message.content, float(np.exp(np.array([t.logprob for t in response.choices[0].logprobs.content])).mean()), [t.logprob for t in response.choices[0].logprobs.content]\n",
    "                    except:\n",
    "                        return response.choices[0].message.content, float(np.exp(response.choices[0].logprobs.token_logprobs).mean()), response.choices[0].logprobs.token_logprobs\n",
    "                else:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=message_list,\n",
    "                        temperature=self.temperature,\n",
    "                        max_tokens=self.max_tokens\n",
    "                    )\n",
    "                    return response.choices[0].message.content, None, None\n",
    "                \n",
    "            # NOTE: BadRequestError is triggered once for MMMU, please uncomment if you are reruning MMMU\n",
    "            except openai.BadRequestError as e:\n",
    "                print(\"Bad Request Error\", e)\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                exception_backoff = min(2**trial, 60)  # expontial back off\n",
    "                print(\n",
    "                    f\"Rate limit exception so wait and retry {trial} after {exception_backoff} sec\",\n",
    "                    e,\n",
    "                )\n",
    "                time.sleep(exception_backoff)\n",
    "                trial += 1\n",
    "            # unknown error shall throw exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68ea4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ChatCompletionSampler()\n",
    "c.logprobs=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a4cb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = c([{\"role\": \"user\", \"content\": \"hi\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5fa0da23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[TopLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.000225947),\n",
       "  TopLogprob(token='Hi', bytes=[72, 105], logprob=-8.637153),\n",
       "  TopLogprob(token=' Hello', bytes=[32, 72, 101, 108, 108, 111], logprob=-10.439324),\n",
       "  TopLogprob(token='Hey', bytes=[72, 101, 121], logprob=-11.673375),\n",
       "  TopLogprob(token='Good', bytes=[71, 111, 111, 100], logprob=-12.9118395),\n",
       "  TopLogprob(token='\\n', bytes=[10], logprob=-12.924041),\n",
       "  TopLogprob(token='hello', bytes=[104, 101, 108, 108, 111], logprob=-12.944734),\n",
       "  TopLogprob(token='Greetings', bytes=[71, 114, 101, 101, 116, 105, 110, 103, 115], logprob=-13.420112),\n",
       "  TopLogprob(token='\\n\\n', bytes=[10, 10], logprob=-14.000659),\n",
       "  TopLogprob(token=' \\n\\n', bytes=[32, 10, 10], logprob=-15.666173)],\n",
       " [TopLogprob(token='!', bytes=[33], logprob=-0.0001726703),\n",
       "  TopLogprob(token=',', bytes=[44], logprob=-9.273631),\n",
       "  TopLogprob(token=' there', bytes=[32, 116, 104, 101, 114, 101], logprob=-9.48706),\n",
       "  TopLogprob(token='.', bytes=[46], logprob=-13.088262),\n",
       "  TopLogprob(token=' and', bytes=[32, 97, 110, 100], logprob=-15.085662),\n",
       "  TopLogprob(token='!\\n', bytes=[33, 10], logprob=-15.148305),\n",
       "  TopLogprob(token='!\\n\\n', bytes=[33, 10, 10], logprob=-16.243246),\n",
       "  TopLogprob(token=' how', bytes=[32, 104, 111, 119], logprob=-16.510906),\n",
       "  TopLogprob(token=' !', bytes=[32, 33], logprob=-17.361677),\n",
       "  TopLogprob(token=' friend', bytes=[32, 102, 114, 105, 101, 110, 100], logprob=-17.396004)],\n",
       " [TopLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-0.00013941615),\n",
       "  TopLogprob(token=' What', bytes=[32, 87, 104, 97, 116], logprob=-9.431814),\n",
       "  TopLogprob(token='How', bytes=[72, 111, 119], logprob=-10.187288),\n",
       "  TopLogprob(token=' Is', bytes=[32, 73, 115], logprob=-11.595577),\n",
       "  TopLogprob(token=' how', bytes=[32, 104, 111, 119], logprob=-11.944664),\n",
       "  TopLogprob(token=' Can', bytes=[32, 67, 97, 110], logprob=-12.43943),\n",
       "  TopLogprob(token=' Do', bytes=[32, 68, 111], logprob=-14.319216),\n",
       "  TopLogprob(token=' ', bytes=[32], logprob=-14.64747),\n",
       "  TopLogprob(token=' Welcome', bytes=[32, 87, 101, 108, 99, 111, 109, 101], logprob=-14.86961),\n",
       "  TopLogprob(token=' It', bytes=[32, 73, 116], logprob=-16.134604)],\n",
       " [TopLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-0.017963793),\n",
       "  TopLogprob(token=' are', bytes=[32, 97, 114, 101], logprob=-4.1793714),\n",
       "  TopLogprob(token=' may', bytes=[32, 109, 97, 121], logprob=-5.9977155),\n",
       "  TopLogprob(token=' Can', bytes=[32, 67, 97, 110], logprob=-12.249662),\n",
       "  TopLogprob(token=\"'s\", bytes=[39, 115], logprob=-12.583555),\n",
       "  TopLogprob(token=' is', bytes=[32, 105, 115], logprob=-13.601805),\n",
       "  TopLogprob(token=' May', bytes=[32, 77, 97, 121], logprob=-14.600401),\n",
       "  TopLogprob(token=\"'re\", bytes=[39, 114, 101], logprob=-15.389246),\n",
       "  TopLogprob(token=' I', bytes=[32, 73], logprob=-16.324276),\n",
       "  TopLogprob(token='’s', bytes=[226, 128, 153, 115], logprob=-16.358503)],\n",
       " [TopLogprob(token=' I', bytes=[32, 73], logprob=-5.080963e-06),\n",
       "  TopLogprob(token=' i', bytes=[32, 105], logprob=-12.408847),\n",
       "  TopLogprob(token=' help', bytes=[32, 104, 101, 108, 112], logprob=-14.466838),\n",
       "  TopLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-15.140582),\n",
       "  TopLogprob(token='I', bytes=[73], logprob=-16.375204),\n",
       "  TopLogprob(token=' ', bytes=[32], logprob=-16.888502),\n",
       "  TopLogprob(token=' l', bytes=[32, 108], logprob=-16.991146),\n",
       "  TopLogprob(token=' we', bytes=[32, 119, 101], logprob=-17.334612),\n",
       "  TopLogprob(token=' AI', bytes=[32, 65, 73], logprob=-17.905304),\n",
       "  TopLogprob(token=' ai', bytes=[32, 97, 105], logprob=-18.137358)],\n",
       " [TopLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.08340744),\n",
       "  TopLogprob(token=' help', bytes=[32, 104, 101, 108, 112], logprob=-2.526023),\n",
       "  TopLogprob(token=' be', bytes=[32, 98, 101], logprob=-10.56758),\n",
       "  TopLogprob(token=' support', bytes=[32, 115, 117, 112, 112, 111, 114, 116], logprob=-10.940404),\n",
       "  TopLogprob(token='assist', bytes=[97, 115, 115, 105, 115, 116], logprob=-13.59703),\n",
       "  TopLogprob(token=' ass', bytes=[32, 97, 115, 115], logprob=-14.250606),\n",
       "  TopLogprob(token=' provide', bytes=[32, 112, 114, 111, 118, 105, 100, 101], logprob=-14.641368),\n",
       "  TopLogprob(token=' assistance', bytes=[32, 97, 115, 115, 105, 115, 116, 97, 110, 99, 101], logprob=-14.648839),\n",
       "  TopLogprob(token=' assists', bytes=[32, 97, 115, 115, 105, 115, 116, 115], logprob=-15.530807),\n",
       "  TopLogprob(token=' make', bytes=[32, 109, 97, 107, 101], logprob=-15.741731)],\n",
       " [TopLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=-5.4385737e-06),\n",
       "  TopLogprob(token=' or', bytes=[32, 111, 114], logprob=-12.507681),\n",
       "  TopLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-14.3571205),\n",
       "  TopLogprob(token='?', bytes=[63], logprob=-15.069971),\n",
       "  TopLogprob(token='you', bytes=[121, 111, 117], logprob=-15.738087),\n",
       "  TopLogprob(token=' yo', bytes=[32, 121, 111], logprob=-15.997026),\n",
       "  TopLogprob(token=' y', bytes=[32, 121], logprob=-15.999924),\n",
       "  TopLogprob(token=' ', bytes=[32], logprob=-16.070436),\n",
       "  TopLogprob(token=' You', bytes=[32, 89, 111, 117], logprob=-16.143282),\n",
       "  TopLogprob(token=' and', bytes=[32, 97, 110, 100], logprob=-16.572233)],\n",
       " [TopLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-8.11329e-05),\n",
       "  TopLogprob(token='?', bytes=[63], logprob=-9.431972),\n",
       "  TopLogprob(token=' tod', bytes=[32, 116, 111, 100], logprob=-14.839534),\n",
       "  TopLogprob(token=' Today', bytes=[32, 84, 111, 100, 97, 121], logprob=-15.897811),\n",
       "  TopLogprob(token=' or', bytes=[32, 111, 114], logprob=-16.115772),\n",
       "  TopLogprob(token=' tonight', bytes=[32, 116, 111, 110, 105, 103, 104, 116], logprob=-16.190842),\n",
       "  TopLogprob(token='today', bytes=[116, 111, 100, 97, 121], logprob=-16.333471),\n",
       "  TopLogprob(token=' toda', bytes=[32, 116, 111, 100, 97], logprob=-16.712673),\n",
       "  TopLogprob(token=' on', bytes=[32, 111, 110], logprob=-16.73742),\n",
       "  TopLogprob(token=' with', bytes=[32, 119, 105, 116, 104], logprob=-17.951992)],\n",
       " [TopLogprob(token='?', bytes=[63], logprob=-1.3067608e-05),\n",
       "  TopLogprob(token='?\\n', bytes=[63, 10], logprob=-11.664595),\n",
       "  TopLogprob(token='?\\n\\n', bytes=[63, 10, 10], logprob=-12.400442),\n",
       "  TopLogprob(token=' ?', bytes=[32, 63], logprob=-16.183306),\n",
       "  TopLogprob(token='?\"', bytes=[63, 34], logprob=-16.700375),\n",
       "  TopLogprob(token=' or', bytes=[32, 111, 114], logprob=-17.31623),\n",
       "  TopLogprob(token='?>', bytes=[63, 62], logprob=-17.693308),\n",
       "  TopLogprob(token='？', bytes=[239, 188, 159], logprob=-17.936),\n",
       "  TopLogprob(token='?\\n\\n\\n', bytes=[63, 10, 10, 10], logprob=-18.10851),\n",
       "  TopLogprob(token=\"?'\", bytes=[63, 39], logprob=-18.352192)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219baae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
