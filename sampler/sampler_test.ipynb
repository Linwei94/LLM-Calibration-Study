{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import os\n",
    "OPENAI_SYSTEM_MESSAGE_API = \"You are a helpful assistant.\"\n",
    "OPENAI_SYSTEM_MESSAGE_CHATGPT = (\n",
    "    \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "    + \"\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\"\n",
    ")\n",
    "\n",
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "\n",
    "credentials, _ = default()\n",
    "auth_request = Request()\n",
    "credentials.refresh(auth_request)\n",
    "base_url = \"https://us-central1-aiplatform.googleapis.com/v1beta1/projects/storied-channel-368910/locations/us-central1/endpoints/openapi\"\n",
    "\n",
    "class ChatCompletionSampler():\n",
    "    \"\"\"\n",
    "    Sample from OpenAI's chat completion API\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        system_message: str | None = None,\n",
    "        temperature: float = 0.5,\n",
    "        max_tokens: int = 1024,\n",
    "        base_url=None,\n",
    "        api_key=None,\n",
    "        use_logprobs = False\n",
    "    ):\n",
    "        self.api_key_name = os.environ[\"TOGETHER_API_KEY\"]\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        # using api_key=os.environ.get(\"OPENAI_API_KEY\")  # please set your API_KEY\n",
    "        self.model = model\n",
    "        self.system_message = system_message\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.image_format = \"url\"\n",
    "        self.use_logprobs = use_logprobs\n",
    "\n",
    "        self.top_logprobs = None\n",
    "        self.logit_perplexity = None\n",
    "        self.log_probs = None\n",
    "\n",
    "    def _handle_image(\n",
    "        self, image: str, encoding: str = \"base64\", format: str = \"png\", fovea: int = 768\n",
    "    ):\n",
    "        new_image = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/{format};{encoding},{image}\",\n",
    "            },\n",
    "        }\n",
    "        return new_image\n",
    "\n",
    "    def _handle_text(self, text: str):\n",
    "        return {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    def _pack_message(self, role: str, content: Any):\n",
    "        return {\"role\": str(role), \"content\": content}\n",
    "\n",
    "    def __call__(self, message_list) -> str:\n",
    "        if self.system_message:\n",
    "            message_list = [self._pack_message(\"system\", self.system_message)] + message_list\n",
    "        trial = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if self.logprobs:\n",
    "                    # together\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=message_list,\n",
    "                        temperature=self.temperature,\n",
    "                        max_tokens=self.max_tokens,\n",
    "                        logprobs=5\n",
    "                    )\n",
    "                    self.top_logprobs = response.choices[0].logprobs.top_logprobs\n",
    "                    return response.choices[0].message.content, float(np.exp(response.choices[0].logprobs.token_logprobs).mean()), response.choices[0].logprobs.token_logprobs, response\n",
    "                else:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=message_list,\n",
    "                        temperature=self.temperature,\n",
    "                        max_tokens=self.max_tokens\n",
    "                    )\n",
    "                    return response.choices[0].message.content, None, None\n",
    "                \n",
    "            # NOTE: BadRequestError is triggered once for MMMU, please uncomment if you are reruning MMMU\n",
    "            except openai.BadRequestError as e:\n",
    "                print(\"Bad Request Error\", e)\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                exception_backoff = min(2**trial, 60)  # expontial back off\n",
    "                print(\n",
    "                    f\"Rate limit exception so wait and retry {trial} after {exception_backoff} sec\",\n",
    "                    e,\n",
    "                )\n",
    "                time.sleep(exception_backoff)\n",
    "                trial += 1\n",
    "            # unknown error shall throw exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ed2ffa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "google = ChatCompletionSampler(\n",
    "            model=\"meta/llama-4-maverick-17b-128e-instruct-maas\",\n",
    "            system_message=OPENAI_SYSTEM_MESSAGE_API,\n",
    "            max_tokens=2048,\n",
    "            base_url=f\"https://{\"us-east5-aiplatform.googleapis.com\"}/v1beta1/projects/storied-channel-368910/locations/us-central1/endpoints/openapi\",\n",
    "            api_key=credentials.token\n",
    "        )\n",
    "\n",
    "to = ChatCompletionSampler(\n",
    "            base_url = \"https://api.together.xyz/v1\",\n",
    "            api_key = os.environ['TOGETHER_API_KEY'],\n",
    "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "            system_message=OPENAI_SYSTEM_MESSAGE_API,\n",
    "            max_tokens=2048,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68ea4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "to.logprobs=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a4cb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = to([{\"role\": \"user\", \"content\": \"hi\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4130b954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
       " 0.9703521864702449,\n",
       " [-1.140625,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -3.5762787e-07,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -0.0015029907,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1.66893e-05],\n",
       " ChatCompletion(id='nrEJLr9-4Yz4kd-934c95111c4ae7cc', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=None, refusal=None, token_ids=[2181, 596, 6555, 311, 3449, 499, 13, 2209, 1070, 2555, 358, 649, 1520, 499, 449, 477, 1053, 499, 1093, 311, 6369, 30, 128009], tokens=['It', \"'s\", ' nice', ' to', ' meet', ' you', '.', ' Is', ' there', ' something', ' I', ' can', ' help', ' you', ' with', ' or', ' would', ' you', ' like', ' to', ' chat', '?', '<|eot_id|>'], token_logprobs=[-1.140625, 0, 0, 0, 0, 0, 0, 0, 0, -3.5762787e-07, 0, 0, 0, 0, 0, -0.0015029907, 0, 0, 0, 0, 0, 0, -1.66893e-05], top_logprobs=[{'Hello': -0.38867188, 'It': -1.140625, 'How': -7.625, 'Hi': -7.875, 'I': -17.625}, {\"'s\": 0, ' nice': -41.5, '’s': -45.75, \"'\": -45.75, ' seems': -46}, {' nice': 0, ' lovely': -21, ' great': -28, 'nice': -31.25, ' Nice': -33.5}, {' to': 0, ' for': -43.75, ' talking': -44.5, ' meeting': -45.75, ' that': -47}, {' meet': 0, ' talk': -30, ' connect': -31.75, ' chat': -31.75, ' meets': -32.75}, {' you': 0, '你': -49.5, 'you': -50.5, ' bạn': -54.25, ' You': -54.5}, {'.': 0, ',': -41.5, '!': -44.5, ' and': -50.75, ' to': -52}, {' Is': 0, ' How': -17.5, ' I': -30.25, ' is': -34.5, ' If': -34.75}, {' there': 0, ' There': -48.25, 'there': -48.5, ' theres': -54.5, ' THERE': -56.75}, {' something': -3.5762787e-07, ' anything': -15, ' Something': -34.25, 'something': -38.25, 'Something': -39.5}, {' I': 0, ' on': -32.25, ' you': -36, ' specific': -37.75, ' particular': -48.5}, {' can': 0, 'can': -42.75, '可以': -47.5, ' Can': -48, ' dapat': -52.25}, {' help': 0, ' assist': -20, ' helps': -33.5, 'help': -35.25, ' helping': -37.5}, {' you': 0, ' with': -40.25, '你': -45, 'you': -46.25, ' bạn': -47}, {' with': 0, 'with': -47.75, ' dengan': -48.25, ' với': -48.75, ' With': -49}, {' or': -0.0015029907, ',': -6.5, ' today': -24.5, 'today': -50.5, '或': -52.75}, {' would': 0, ' Would': -25.75, 'would': -32, 'Would': -33, ' perhaps': -34}, {' you': 0, '你': -51, 'you': -54, ' bạn': -55.5, ' você': -56.75}, {' like': 0, ' just': -30.75, ' Like': -42, 'like': -44.25, ' liked': -47}, {' to': 0, ' just': -46.75, ' some': -51, '\\tto': -57.25, ' someone': -57.5}, {' chat': 0, ' just': -22.5, ' chatting': -26, ' Chat': -27.25, 'chat': -27.75}, {'?': 0, ' for': -19.75, ' about': -35, '?.': -35.75, '?\\n': -38.5}, {'<|eot_id|>': -1.66893e-05, ' I': -11, 'EMPLARY': -23.5, '<|eom_id|>': -24.25, '<|python_tag|>': -26}]), message=ChatCompletionMessage(content=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=1494433389348482800)], created=1745403144, model='meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=42, total_tokens=65, completion_tokens_details=None, prompt_tokens_details=None, cached_tokens=0), prompt=[]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa89254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
