{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb2f0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "OPENAI_SYSTEM_MESSAGE_API = \"You are a helpful assistant.\"\n",
    "OPENAI_SYSTEM_MESSAGE_CHATGPT = (\n",
    "    \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "    + \"\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\"\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCompletionSampler():\n",
    "    \"\"\"\n",
    "    Sample from OpenAI's chat completion API\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        system_message: str | None = None,\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = 1024,\n",
    "        base_url=None,\n",
    "        api_key=None,\n",
    "        get_logprobs = False\n",
    "    ):\n",
    "        # self.api_key_name = \"OPENAI_API_KEY\"\n",
    "        if base_url:\n",
    "            if any(provider in base_url for provider in [\"google\", \"databricks\", \"together\", \"deepseek\", \"local\"]):\n",
    "                self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        else:\n",
    "            OpenAI.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "            self.client = OpenAI()\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.system_message = system_message\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.image_format = \"url\"\n",
    "        self.get_logprobs = get_logprobs\n",
    "        self.logprobs = None\n",
    "        self.top_logprobs = None\n",
    "\n",
    "    def _handle_image(\n",
    "        self, image: str, encoding: str = \"base64\", format: str = \"png\", fovea: int = 768\n",
    "    ):\n",
    "        new_image = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/{format};{encoding},{image}\",\n",
    "            },\n",
    "        }\n",
    "        return new_image\n",
    "\n",
    "    def _handle_text(self, text: str):\n",
    "        return {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    def _pack_message(self, role: str, content: Any):\n",
    "        return {\"role\": str(role), \"content\": content}\n",
    "\n",
    "    def __call__(self, message_list) -> str:\n",
    "        if self.system_message:\n",
    "            message_list = [self._pack_message(\"system\", self.system_message)] + message_list\n",
    "        trial = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if self.get_logprobs:\n",
    "                    if self.base_url and \"together\" in self.base_url:\n",
    "                        print(self.model, \"Together API\")\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            model=self.model,\n",
    "                            messages=message_list,\n",
    "                            temperature=self.temperature,\n",
    "                            max_tokens=self.max_tokens,\n",
    "                            logprobs=True,\n",
    "                            top_logprobs=5,\n",
    "                            seed=42,\n",
    "                        )\n",
    "                        try:\n",
    "                            self.top_logprobs = response.choices[0].logprobs.top_logprobs # a list of dicts each of which is a dict of possible candidates with its logprob\n",
    "                        except:\n",
    "                            print(self.model, \"Top logprobs not found\")\n",
    "                        try:\n",
    "                            self.logprobs = response.choices[0].logprobs.token_logprobs\n",
    "                        except:\n",
    "                            print(self.model, \"Token logprobs not found\")\n",
    "\n",
    "                        if self.model == \"meta-llama/Llama-2-70b-hf\":\n",
    "                            print(response)\n",
    "                        self.raw_response = response\n",
    "                        return response.choices[0].message.content\n",
    "                    \n",
    "                    elif self.base_url and \"databricks\" in self.base_url:\n",
    "                        print(self.model, \"Databricks API\")\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            messages=message_list, \n",
    "                            model=self.model, \n",
    "                            max_tokens=self.max_tokens, \n",
    "                            logprobs=True,\n",
    "                            top_logprobs=5,\n",
    "                            temperature=0\n",
    "                        )\n",
    "                        self.top_logprobs = [t.top_logprobs for t in response.choices[0].logprobs.content]\n",
    "                        self.logprobs = [t.logprob for t in response.choices[0].logprobs.content]\n",
    "                        return response.choices[0].message.content\n",
    "                    \n",
    "                    elif self.base_url and \"local\" in self.base_url:\n",
    "                        print(self.model, \"vLLM API\")\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            messages=message_list, \n",
    "                            model=self.model, \n",
    "                            max_tokens=self.max_tokens, \n",
    "                            logprobs=True,\n",
    "                            top_logprobs=10,\n",
    "                            temperature=0,\n",
    "                            seed=42,\n",
    "                        )\n",
    "                        top_logprob_lst = []\n",
    "                        for top_list in [t.top_logprobs for t in response.choices[0].logprobs.content]: \n",
    "                            top_logprob_lst.append({t.token: t. logprob for t in top_list})\n",
    "                        self.top_logprobs = top_logprob_lst\n",
    "                        self.logprobs = [t.logprob for t in response.choices[0].logprobs.content]\n",
    "                        return response.choices[0].message.content\n",
    "                    \n",
    "                    elif self.base_url and \"deepseek\" in self.base_url: \n",
    "                        print(self.model, \"Deep Seek API\")\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            messages=message_list, \n",
    "                            model=self.model, \n",
    "                            max_tokens=self.max_tokens, \n",
    "                            logprobs=True,\n",
    "                            top_logprobs=5,\n",
    "                            temperature=0,\n",
    "                            seed=42\n",
    "                        )\n",
    "                        top_logprob_lst = []\n",
    "                        for top_list in [t.top_logprobs for t in response.choices[0].logprobs.content]: \n",
    "                            top_logprob_lst.append({t.token: t. logprob for t in top_list})\n",
    "                        self.top_logprobs = top_logprob_lst\n",
    "                        self.logprobs = [t.logprob for t in response.choices[0].logprobs.content]\n",
    "                        return response.choices[0].message.content\n",
    "                    \n",
    "                    else:\n",
    "                        print(self.model, \"OpenAI API\")\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            messages=message_list, \n",
    "                            model=self.model, \n",
    "                            max_tokens=self.max_tokens, \n",
    "                            logprobs=True,\n",
    "                            top_logprobs=5,\n",
    "                            temperature=0,\n",
    "                            seed=42\n",
    "                        )\n",
    "                        top_logprob_lst = []\n",
    "                        for top_list in [t.top_logprobs for t in response.choices[0].logprobs.content]: \n",
    "                            top_logprob_lst.append({t.token: t. logprob for t in top_list})\n",
    "                        self.top_logprobs = top_logprob_lst\n",
    "                        self.logprobs = [t.logprob for t in response.choices[0].logprobs.content]\n",
    "                        return response.choices[0].message.content\n",
    "\n",
    "                else:\n",
    "                    print(self.model, \"No logprobs\")\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=message_list,\n",
    "                        temperature=self.temperature,\n",
    "                        max_tokens=self.max_tokens\n",
    "                    )\n",
    "                    return response.choices[0].message.content\n",
    "                \n",
    "            # NOTE: BadRequestError is triggered once for MMMU, please uncomment if you are reruning MMMU\n",
    "            except openai.BadRequestError as e:\n",
    "                print(\"Bad Request Error\", e)\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                exception_backoff = min(2**trial, 60)  # expontial back off\n",
    "                print(\n",
    "                    f\"Rate limit exception so wait and retry {trial} after {exception_backoff} sec\",\n",
    "                    e,\n",
    "                )\n",
    "                time.sleep(exception_backoff)\n",
    "                trial += 1\n",
    "            # unknown error shall throw exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed2ffa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to = ChatCompletionSampler(\n",
    "            base_url = \"https://api.together.xyz/v1\",\n",
    "            api_key = os.environ['TOGETHER_API_KEY'],\n",
    "            model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "            system_message=OPENAI_SYSTEM_MESSAGE_API,\n",
    "            max_tokens=2048,\n",
    "            get_logprobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a4cb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo Together API\n"
     ]
    }
   ],
   "source": [
    "response = to([{\"role\": \"user\", \"content\": \"hi how are you\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6134c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always happy to help with any questions or tasks you may have. How about you? How's your day going so far?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4130b954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'I': -0.35742188, 'Hello': -1.234375, 'Hi': -4.71875, 'It': -9.875, '*I': -10.25}, {\"'m\": -6.3180923e-06, ' am': -12.25, ' don': -14.5, \"'d\": -15.25, ' hope': -15.8125}, {' doing': -0.10107422, ' just': -2.46875, ' here': -4.71875, ' good': -6.84375, ' a': -7.09375}, {' well': -0.07910156, ' great': -2.578125, ' fine': -13.9375, ' very': -14.6875, ' just': -14.6875}, {',': -0.00074005127, '.': -7.25, ' thank': -10.625, ' thanks': -12.375, ' and': -15.875}, {' thank': -0.048583984, ' thanks': -3.046875, ' and': -14.4375, 'thank': -15.6875, ' I': -15.9375}, {' you': 0, '.': -19.125, ' You': -19.5, ' for': -19.625, ' y': -21.25}, {' for': -0.0004310608, '.': -7.75, '!': -17.625, ' so': -19.375, 'for': -21.75}, {' asking': 0, ' ask': -17.375, ' asked': -18, ' checking': -18.125, ' asks': -19.125}, {'.': -0.0005569458, '!': -7.5, '?': -14, ' I': -15, '.I': -15.1875}, {' I': -0.020751953, ' How': -4.53125, ' It': -4.65625, ' Is': -12.375, ' how': -13}, {\"'m\": -5.00679e-06, ' hope': -12.75, ' am': -13.875, \"'s\": -13.875, \"'re\": -15.875}, {' a': -0.578125, ' here': -0.828125, ' happy': -7.3125, ' ready': -9.8125, ' always': -10.0625}, {' large': -0.390625, ' helpful': -1.640625, ' computer': -2.140625, ' friendly': -4.65625, ' language': -7.03125}, {' language': -1.5497208e-06, ' computer': -13.375, ' AI': -17.5, ' Language': -18.5, ' machine': -19.125}, {' model': -2.9802322e-06, ' assistant': -12.75, ' AI': -17.75, ' Model': -18, ' mode': -18.25}, {',': -3.695488e-06, ' designed': -12.625, ' so': -14.875, ' assistant': -16.625, ' here': -19}, {' so': -5.221367e-05, ' I': -9.875, ' and': -14.875, ' my': -15.25, ' designed': -16}, {' I': 0, ' don': -21.625, ' i': -23, ' you': -23.625, ' it': -23.75}, {' don': 0, ' dont': -18.625, \"'m\": -19, ' doesn': -20, ' do': -20.375}, {\"'t\": -1.1920929e-07, '’t': -16.25, \"'\": -18, \"'s\": -18.375, \" '\": -20}, {' have': -3.8146973e-06, ' experience': -13.125, ' feel': -13.25, ' feelings': -19.5, ' really': -21.375}, {' feelings': -0.47460938, ' emotions': -0.97265625, ' feeling': -12.75, ' feel': -12.75, ' personal': -14}, {' or': -0.23730469, ' like': -1.734375, ' in': -3.359375, ' and': -9.25, ',': -11.25}, {' emotions': -4.887581e-06, ' experiences': -13.125, ' personal': -13.25, ' physical': -13.75, ' Em': -18}, {' like': -0.0025634766, ',': -6, ' in': -9.375, ' the': -15.25, ' likes': -16.875}, {' humans': -1.1920929e-07, ' a': -16.125, ' people': -17.125, ' Humans': -20.5, ' human': -21}, {' do': -3.695488e-06, ',': -12.5, 'do': -23.75, ' Do': -24.625, ' did': -26.375}, {',': -2.2649765e-06, '.': -13, ' but': -22.25, ';': -24.125, ' But': -25.25}, {' but': 0, ' But': -22.625, ' BUT': -24.5, ' nhưng': -25.125, ' I': -25.375}, {' I': -1.1920929e-07, ' it': -15.625, ' i': -24.75, ' It': -25.5, ' my': -25.75}, {\"'m\": 0, \"'s\": -17.125, \"'re\": -17.625, ' appreciate': -18.5, ' am': -18.75}, {' always': -0.23730469, ' here': -1.734375, ' functioning': -3.359375, ' happy': -8.875, ' ready': -10.5}, {' happy': -0.015136719, ' ready': -4.875, ' here': -4.875, ' glad': -12.75, ' excited': -13.5}, {' to': 0, ' and': -21.625, ' when': -25.75, ' (': -27.125, ' chat': -27.25}, {' help': -0.6953125, ' chat': -0.6953125, ' assist': -6.6875, ' be': -8.9375, ' helps': -12.9375}, {' with': -0.24316406, ' and': -2.125, ' answer': -2.375, ' you': -6.25, ' or': -14.125}, {' any': -4.7683716e-07, ' questions': -14.5, ' anything': -17.75, ' whatever': -20.125, 'any': -22.875}, {' questions': -1.1920929e-07, ' question': -15.875, ' tasks': -22.125, ' information': -22.375, ' queries': -23}, {' or': 0, ' you': -17.5, ',': -21.5, ' (': -23.75, ' and': -24.5}, {' tasks': -0.08935547, ' topics': -2.46875, ' problems': -8.3125, ' concerns': -10.6875, ' chat': -10.6875}, {' you': 0, ' that': -19.75, ' You': -20.5, '.': -20.5, 'you': -20.75}, {' may': -0.27539062, ' have': -1.8984375, ' might': -2.40625, ' need': -10.75, ' May': -15.875}, {' have': 0, ' need': -17.375, 'have': -24.25, ' Have': -25.25, ' having': -26.625}, {'.': -9.775162e-05, '.\\n\\n': -9.375, '!': -11.25, ' How': -18.625, '.\\n': -18.625}, {' How': -0.00049209595, ' Is': -8.25, ' It': -8.5, ' What': -10.625, ' how': -12.625}, {' about': -0.100097656, ' can': -2.34375, ' are': -11.5, 'about': -16.5, ' Can': -16.875}, {' you': 0, ' yourself': -25.625, ' You': -27.375, ' YOU': -27.5, 'you': -28.125}, {'?': -4.2915344e-06, ',': -12.375, ' -': -16, ';': -20.875, ':': -20.875}, {' How': -0.008666992, ' Is': -4.75, ' What': -10.25, ' how': -13.5, '<|eot_id|>': -17.875}, {\"'s\": -0.00020313263, ' can': -8.5, ' are': -15.5, ' is': -17, ' was': -20}, {' your': 0, 'your': -24, ' Your': -24, ' you': -24.625, ' our': -26.625}, {' day': 0, ' Day': -28, ' today': -28.5, 'day': -28.625, ' week': -29.5}, {' going': 0, ' so': -16.75, ' been': -17.25, ' go': -22.125, 'going': -23.125}, {' so': -0.14257812, '?': -2.015625, ' today': -13.0625, '?\\n': -13.4375, ' So': -14.75}, {' far': 0, 'far': -21.5, ' fa': -24.375, ' Far': -24.5, '_far': -25.375}, {'?': 0, '?\"': -20.75, '?\\n': -21, '?)': -21.25, '?\\n\\n': -21.375}, {'<|eot_id|>': -0.033691406, ' Is': -3.40625, ' Do': -11.9375, '<|eom_id|>': -12.6875, ' is': -12.6875}]\n"
     ]
    }
   ],
   "source": [
    "print(to.top_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa89254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
