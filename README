# Source Code for *Revisiting Uncertainty Estimation and Calibration of Large Language Models*

The source code is based on [Simple Evals](https://github.com/openai/simple-evals) by OpenAI. We focus on the MMLU-PRO benchmark and include various samplers and evaluation metrics explored in our paper.

Before running, ensure to set up the environment:

```bash
export HF_TOKEN=<YOUR HUGGING FACE API TOKEN>
export TOGETHER_API_KEY=<YOUR TOGETHER API TOKEN>
export OPENAI_API_KEY=<YOUR OPEN AI API TOKEN>
export DEEPSEEK_API_KEY=<YOUR DEEP SEEK API TOKEN>
export GOOGLE_AI_KEY=<YOUR GOOGLE AI STUDIO API TOKEN>
```

To initiate MMLU-PRO sampling, run:
```bash
python -m LLM-Calibration-Study.simple_evals --model MODEL_NAME --benchmark mmlu_pro --conf_mode sampling
```

Should a model is not supported by vLLM, run:
```bash
python -m LLM-Calibration-Study.simple_evals --model MODEL_NAME --benchmark mmlu_pro --conf_mode sampling-hf
```

To initiate MMLU-PRO evaluation after sampling:
```bash
python -m LLM-Calibration-Study.simple_evals --model MODEL_NAME --benchmark mmlu_pro --conf_mode eval_all
```

where `MODEL_NAME` is one of:
```
Qwen/Qwen3-14B-AWQ
Qwen/Qwen3-32B-AWQ
Qwen/Qwen2.5-3B-Instruct
Qwen/Qwen2.5-7B-Instruct
Qwen/Qwen2.5-14B-Instruct
Qwen/Qwen2.5-32B-Instruct
Qwen/Qwen3-0.6B
Qwen/Qwen3-0.6B-FP8
Qwen/Qwen3-0.6B-Base
Qwen/Qwen3-1.7B-FP8
Qwen/Qwen3-1.7B
Qwen/Qwen3-1.7B-Base
Qwen/Qwen3-4B-FP8
Qwen/Qwen3-4B
Qwen/Qwen3-4B-Base
Qwen/Qwen3-8B-FP8
Qwen/Qwen3-8B
Qwen/Qwen3-8B-Base
Qwen/Qwen3-14B-FP8
Qwen/Qwen3-14B-Base
Qwen/Qwen3-14B
Qwen/Qwen3-32B
Qwen/Qwen3-30B-A3B
Qwen/Qwen3-30B-A3B-Base
Qwen/Qwen3-0.6B-think
Qwen/Qwen3-1.7B-think
Qwen/Qwen3-4B-think
Qwen/Qwen3-8B-think
Qwen/Qwen3-14B-think
Qwen/Qwen3-32B-think
Qwen/Qwen3-30B-A3B-think
google/gemma-3-27b-it
google/gemma-2-27b-it
google/gemma-2-9b-it
google/gemma-2b-it
google/gemma-3-1b-it
google/gemma-3-4b-it
google/gemma-3-12b-it
mistralai/Mistral-7B-Instruct-v0.1
mistralai/Mistral-Small-3.1-24B-Base-2503
mistralai/Mistral-Large-Instruct-2411
mistralai/Mistral-Nemo-Instruct-2407
nvidia/Llama-3.1-Nemotron-Nano-8B-v1
nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1
microsoft/Phi-4-mini-reasoning
microsoft/Phi-4-mini-instruct
microsoft/Phi-4-reasoning
microsoft/Phi-3.5-mini-instruct
microsoft/Phi-3.5-MoE-instruct
microsoft/Phi-3-mini-128k-instruct
microsoft/Phi-3-small-128k-instruct
microsoft/Phi-3-medium-128k-instruct
gpt-4.1-mini
gpt-4.1-nano
gpt-4.1
gpt-4o-2024-08-06
gpt-4o-mini-2024-07-18
o1-mini-2024-09-12
o3-mini-2025-01-31
o4-mini-2025-04-16
meta-llama/Llama-4-Scout-17B-16E-Instruct
meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
meta-llama/Llama-3.2-3B-Instruct-Turbo
meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
Qwen/Qwen3-235B-A22B-fp8-tput-think
Qwen/Qwen3-235B-A22B-fp8-tput
mistralai/Mistral-Small-24B-Instruct-2501
deepseek-chat
deepseek-reasoner
claude-3-7-sonnet-20250219
claude-3-5-haiku-20241022
claude-3-haiku-20240307
grok-3-beta
grok-3-mini-beta
grok-2-1212
gemini-2.0-flash
gemini-2.0-flash-lite
gemini-1.5-flash
gemini-1.5-flash-8b
gemini-1.5-pro
```