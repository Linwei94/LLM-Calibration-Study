{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6101d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13d219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ece(confidences, accuracies, n_bins=10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the expected calibration error (ECE) given a list of confidence scores (0-1) and accuracy scores (0 or 1).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"conf\": confidences, \"acc\": accuracies}).dropna()\n",
    "\n",
    "    confidences = torch.tensor(df[\"conf\"].tolist())\n",
    "    accuracies = torch.tensor(df[\"acc\"].tolist())\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = torch.zeros(1)\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Calculated |confidence - accuracy| in each bin\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16202836",
   "metadata": {},
   "source": [
    "# Drop NA on an Individual Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1af877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Missing answer</th>\n",
       "      <th>Missing verbal numerical confidence</th>\n",
       "      <th>Missing logit perplexity confidence</th>\n",
       "      <th>Missing verbal linguistic confidence</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy without na</th>\n",
       "      <th>ECE verbal numerical confidence</th>\n",
       "      <th>ECE logit perplexity confidence</th>\n",
       "      <th>ECE verbal linguistic confidence</th>\n",
       "      <th>AUROC verbal numerical confidence</th>\n",
       "      <th>AUROC logit perplexity confidence</th>\n",
       "      <th>AUROC verbal linguistic confidence</th>\n",
       "      <th>Mean verbal numerical confidence</th>\n",
       "      <th>Mean logit perplexity confidence</th>\n",
       "      <th>Mean verbal linguistic confidence</th>\n",
       "      <th>Std verbal numerical confidence</th>\n",
       "      <th>Std logit perplexity confidence</th>\n",
       "      <th>Std verbal linguistic confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gpt-4.1</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.787899</td>\n",
       "      <td>0.789868</td>\n",
       "      <td>0.176778</td>\n",
       "      <td>0.077492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750999</td>\n",
       "      <td>0.678905</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.965989</td>\n",
       "      <td>0.865391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059353</td>\n",
       "      <td>0.059776</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gpt-4.1-mini</td>\n",
       "      <td>180</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.749418</td>\n",
       "      <td>0.760800</td>\n",
       "      <td>0.168441</td>\n",
       "      <td>0.141744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778065</td>\n",
       "      <td>0.715169</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.928326</td>\n",
       "      <td>0.891108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062468</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gpt-4.1-nano</td>\n",
       "      <td>405</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613946</td>\n",
       "      <td>0.635332</td>\n",
       "      <td>0.256408</td>\n",
       "      <td>0.235984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.721709</td>\n",
       "      <td>0.680920</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.876526</td>\n",
       "      <td>0.849930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071456</td>\n",
       "      <td>0.058216</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gpt-4o-2024-08-06</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.722490</td>\n",
       "      <td>0.725747</td>\n",
       "      <td>0.208816</td>\n",
       "      <td>0.128811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708700</td>\n",
       "      <td>0.639961</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.932718</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079690</td>\n",
       "      <td>0.061558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gpt-4o-2024-08-06_eval_all_3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.271302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.892940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.038240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gpt-4o-mini-2024-07-18</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599069</td>\n",
       "      <td>0.600617</td>\n",
       "      <td>0.287581</td>\n",
       "      <td>0.241333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.739955</td>\n",
       "      <td>0.674036</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.886899</td>\n",
       "      <td>0.840402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.061092</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O1-mini-2024-09-12</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.712932</td>\n",
       "      <td>0.713169</td>\n",
       "      <td>0.218326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776366</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.931339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O3-mini-2025-01-31</td>\n",
       "      <td>421</td>\n",
       "      <td>429</td>\n",
       "      <td>12032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.768201</td>\n",
       "      <td>0.796055</td>\n",
       "      <td>0.160785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793498</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.957045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O4-mini-2025-04-16</td>\n",
       "      <td>239</td>\n",
       "      <td>237</td>\n",
       "      <td>12032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815409</td>\n",
       "      <td>0.831934</td>\n",
       "      <td>0.083412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799996</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.914043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model  Missing answer  \\\n",
       "0                           Gpt-4.1              30   \n",
       "1                      Gpt-4.1-mini             180   \n",
       "2                      Gpt-4.1-nano             405   \n",
       "3                 Gpt-4o-2024-08-06              54   \n",
       "4  Gpt-4o-2024-08-06_eval_all_3.csv               0   \n",
       "5            Gpt-4o-mini-2024-07-18              31   \n",
       "6                O1-mini-2024-09-12               4   \n",
       "7                O3-mini-2025-01-31             421   \n",
       "8                O4-mini-2025-04-16             239   \n",
       "\n",
       "   Missing verbal numerical confidence  Missing logit perplexity confidence  \\\n",
       "0                                   20                                    0   \n",
       "1                                  171                                    0   \n",
       "2                                  132                                    0   \n",
       "3                                   29                                    0   \n",
       "4                                    0                                    0   \n",
       "5                                    9                                    0   \n",
       "6                                    5                                12032   \n",
       "7                                  429                                12032   \n",
       "8                                  237                                12032   \n",
       "\n",
       "   Missing verbal linguistic confidence  Accuracy  Accuracy without na  \\\n",
       "0                                     0  0.787899             0.789868   \n",
       "1                                     0  0.749418             0.760800   \n",
       "2                                     0  0.613946             0.635332   \n",
       "3                                     0  0.722490             0.725747   \n",
       "4                                     0  0.666667             0.666667   \n",
       "5                                     0  0.599069             0.600617   \n",
       "6                                     0  0.712932             0.713169   \n",
       "7                                     0  0.768201             0.796055   \n",
       "8                                     0  0.815409             0.831934   \n",
       "\n",
       "   ECE verbal numerical confidence  ECE logit perplexity confidence  \\\n",
       "0                         0.176778                         0.077492   \n",
       "1                         0.168441                         0.141744   \n",
       "2                         0.256408                         0.235984   \n",
       "3                         0.208816                         0.128811   \n",
       "4                         0.316667                         0.271302   \n",
       "5                         0.287581                         0.241333   \n",
       "6                         0.218326                         0.000000   \n",
       "7                         0.160785                         0.000000   \n",
       "8                         0.083412                         0.000000   \n",
       "\n",
       "   ECE verbal linguistic confidence  AUROC verbal numerical confidence  \\\n",
       "0                               0.0                           0.750999   \n",
       "1                               0.0                           0.778065   \n",
       "2                               0.0                           0.721709   \n",
       "3                               0.0                           0.708700   \n",
       "4                               0.0                           1.000000   \n",
       "5                               0.0                           0.739955   \n",
       "6                               0.0                           0.776366   \n",
       "7                               0.0                           0.793498   \n",
       "8                               0.0                           0.799996   \n",
       "\n",
       "   AUROC logit perplexity confidence  AUROC verbal linguistic confidence  \\\n",
       "0                           0.678905                                 0.5   \n",
       "1                           0.715169                                 0.5   \n",
       "2                           0.680920                                 0.5   \n",
       "3                           0.639961                                 0.5   \n",
       "4                           1.000000                                 0.5   \n",
       "5                           0.674036                                 0.5   \n",
       "6                           0.500000                                 0.5   \n",
       "7                           0.500000                                 0.5   \n",
       "8                           0.500000                                 0.5   \n",
       "\n",
       "   Mean verbal numerical confidence  Mean logit perplexity confidence  \\\n",
       "0                          0.965989                          0.865391   \n",
       "1                          0.928326                          0.891108   \n",
       "2                          0.876526                          0.849930   \n",
       "3                          0.932718                          0.851301   \n",
       "4                          0.983333                          0.892940   \n",
       "5                          0.886899                          0.840402   \n",
       "6                          0.931339                               NaN   \n",
       "7                          0.957045                               NaN   \n",
       "8                          0.914043                               NaN   \n",
       "\n",
       "   Mean verbal linguistic confidence  Std verbal numerical confidence  \\\n",
       "0                                0.0                         0.059353   \n",
       "1                                0.0                         0.062468   \n",
       "2                                0.0                         0.071456   \n",
       "3                                0.0                         0.079690   \n",
       "4                                0.0                         0.028868   \n",
       "5                                0.0                         0.056000   \n",
       "6                                0.0                         0.075349   \n",
       "7                                0.0                         0.061770   \n",
       "8                                0.0                         0.090422   \n",
       "\n",
       "   Std logit perplexity confidence  Std verbal linguistic confidence  \n",
       "0                         0.059776                               0.0  \n",
       "1                         0.049969                               0.0  \n",
       "2                         0.058216                               0.0  \n",
       "3                         0.061558                               0.0  \n",
       "4                         0.038240                               0.0  \n",
       "5                         0.061092                               0.0  \n",
       "6                              NaN                               0.0  \n",
       "7                              NaN                               0.0  \n",
       "8                              NaN                               0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_df = pd.DataFrame()\n",
    "for file in os.listdir(\"../results\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        df = pd.read_csv(\"../results/\" + file)\n",
    "        acc_no_na = df[[\"extracted_answer\", \"correct_answer\"]].dropna()\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Model\": [file.replace(\"mmlu_pro_\", \"\").replace(\"_eval_all_None.csv\", \"\").strip().capitalize()],\n",
    "            \"Missing answer\": [df[\"extracted_answer\"].isna().sum()],\n",
    "            \"Missing verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].isna().sum()],\n",
    "            \"Missing logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].isna().sum()],\n",
    "            \"Missing verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].isna().sum()],\n",
    "            \"Accuracy\": [(df[\"extracted_answer\"] == df[\"correct_answer\"]).mean()],\n",
    "            \"Accuracy without na\": [(acc_no_na[\"extracted_answer\"] == acc_no_na[\"correct_answer\"]).mean()],\n",
    "            \"ECE verbal numerical confidence\": [calculate_ece(df[\"verbal_numerical_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"ECE logit perplexity confidence\": [calculate_ece(df[\"logit_perplexity_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"ECE verbal linguistic confidence\": [calculate_ece(df[\"verbal_linguistic_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"AUROC verbal numerical confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]).values, df[\"verbal_numerical_confidence\"].fillna(0).values),\n",
    "            \"AUROC logit perplexity confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]), df[\"logit_perplexity_confidence\"].fillna(0).values),\n",
    "            \"AUROC verbal linguistic confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]), df[\"verbal_linguistic_confidence\"].fillna(0).values),\n",
    "            \"Mean verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].mean()],\n",
    "            \"Mean logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].mean()],\n",
    "            \"Mean verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].mean()],\n",
    "            \"Std verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].std()],\n",
    "            \"Std logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].std()],\n",
    "            \"Std verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].std()],\n",
    "        })\n",
    "        stats_df = pd.concat([stats_df, new_row], ignore_index=True)\n",
    "    \n",
    "display(stats_df.sort_values(by=\"Model\", ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9228210",
   "metadata": {},
   "source": [
    "# Drop NA Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d3eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_indices = set()\n",
    "\n",
    "for file in os.listdir(\"../results\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        df = pd.read_csv(\"../results/\" + file)\n",
    "\n",
    "        # Find indices where either column has NaN\n",
    "        indices_with_nan = df[\n",
    "            df[\"extracted_answer\"].isna() | df[\"verbal_numerical_confidence\"].isna()\n",
    "        ].index\n",
    "\n",
    "        # Combine indices across all files\n",
    "        nan_indices.update(indices_with_nan)\n",
    "\n",
    "# Convert to sorted list if needed\n",
    "nan_indices = sorted(nan_indices)\n",
    "len(nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248404fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[18, 35, 39, 51, 64, 68, 70, 74, 104, 137, 141, 157, 172, 199, 200, 249, 252, 267, 268, 331, 332, 338, 339, 371, 382, 395, 400, 405, 407, 416, 420, 422, 435, 451, 452, 485, 522, 540, 568, 579, 589, 590, 595, 602, 617, 628, 630, 701, 709, 724, 731, 738, 764, 783, 786, 797, 832, 851, 923, 1010, 1209, 1310, 1368, 1432, 1477, 1478, 1484, 1620, 1670, 1686, 1694, 1772, 1806, 1893, 1947, 2019, 2028, 2090, 2711, 2769, 2798, 2825, 2885, 2958, 2966, 3162, 3169, 3210, 3283, 3286, 3336, 3383, 3421, 3435, 3441, 3452, 3459, 3462, 3464, 3468, 3476, 3478, 3490, 3491, 3494, 3495, 3496, 3501, 3522, 3529, 3530, 3531, 3537, 3550, 3552, 3554, 3561, 3564, 3576, 3583, 3587, 3591, 3605, 3614, 3623, 3624, 3636, 3653, 3663, 3666, 3669, 3673, 3682, 3689, 3694, 3715, 3720, 3725, 3736, 3745, 3751, 3754, 3776, 3794, 3796, 3802, 3815, 3825, 3835, 3836, 3846, 3847, 3851, 3853, 3856, 3859, 3868, 3869, 3870, 3875, 3888, 3890, 3900, 3902, 3913, 3930, 3935, 3939, 3941, 3964, 3968, 3972, 3974, 3975, 3977, 3983, 3985, 3993, 3994, 3995, 3996, 4002, 4014, 4021, 4026, 4029, 4042, 4044, 4048, 4049, 4055, 4059, 4062, 4063, 4073, 4077, 4082, 4083, 4088, 4092, 4095, 4096, 4097, 4107, 4110, 4112, 4116, 4129, 4136, 4148, 4160, 4169, 4170, 4177, 4182, 4192, 4194, 4199, 4203, 4219, 4224, 4237, 4248, 4256, 4257, 4258, 4266, 4286, 4318, 4333, 4337, 4339, 4349, 4357, 4360, 4365, 4367, 4402, 4417, 4420, 4422, 4427, 4432, 4437, 4456, 4461, 4481, 4488, 4508, 4521, 4523, 4524, 4527, 4603, 4610, 4654, 4754, 4802, 4864, 5062, 5074, 5194, 5196, 5201, 5207, 5247, 5261, 5269, 5561, 5689, 5766, 5866, 5883, 5914, 6038, 6060, 6157, 6380, 6406, 6466, 6492, 6676, 6713, 6915, 6996, 7023, 7094, 7201, 7351, 7391, 7422, 7476, 7480, 7554, 7566, 7569, 7575, 7580, 7588, 7590, 7594, 7598, 7599, 7646, 7655, 7661, 7666, 7669, 7671, 7673, 7674, 7684, 7687, 7689, 7691, 7695, 7696, 7731, 7745, 7756, 7765, 7768, 7774, 7777, 7779, 7783, 7790, 7791, 7796, 7797, 7801, 7808, 7817, 7826, 7853, 7857, 7869, 7886, 7890, 7896, 7897, 7902, 7910, 7915, 7920, 7971, 7990, 7993, 8000, 8001, 8005, 8017, 8026, 8027, 8030, 8031, 8036, 8081, 8094, 8100, 8102, 8106, 8107, 8110, 8114, 8119, 8123, 8124, 8125, 8126, 8129, 8130, 8131, 8136, 8155, 8197, 8213, 8217, 8222, 8230, 8232, 8233, 8237, 8241, 8242, 8244, 8246, 8313, 8317, 8322, 8323, 8324, 8327, 8337, 8339, 8345, 8346, 8349, 8405, 8416, 8422, 8425, 8427, 8428, 8433, 8434, 8443, 8444, 8446, 8452, 8454, 8458, 8524, 8537, 8538, 8539, 8541, 8548, 8554, 8560, 8566, 8574, 8576, 8577, 8609, 8621, 8624, 8625, 8626, 8646, 8647, 8651, 8653, 8655, 8656, 8659, 8660, 8663, 8664, 8668, 8671, 8696, 8744, 8747, 8749, 8750, 8752, 8753, 8754, 8756, 8759, 8762, 8768, 8771, 8775, 8835, 8836, 8843, 8844, 8846, 8878, 8891, 8895, 8897, 8907, 8909, 8910, 8938, 8946, 8948, 8964, 8966, 8969, 8971, 8973, 8974, 8985, 8990, 8997, 8999, 9015, 9017, 9032, 9036, 9037, 9051, 9061, 9077, 9079, 9082, 9084, 9102, 9106, 9136, 9138, 9147, 9148, 9154, 9184, 9189, 9192, 9207, 9250, 9260, 9261, 9265, 9269, 9273, 9278, 9285, 9303, 9317, 9319, 9321, 9322, 9329, 9350, 9365, 9378, 9383, 9384, 9394, 9404, 9405, 9407, 9411, 9415, 9418, 9426, 9431, 9433, 9464, 9479, 9480, 9482, 9494, 9501, 9516, 9556, 9558, 9559, 9563, 9572, 9583, 9593, 9594, 9606, 9618, 9650, 9652, 9665, 9666, 9668, 9694, 9707, 9710, 9716, 9723, 9724, 9726, 9727, 9728, 9731, 9733, 9738, 9765, 9766, 9770, 9775, 9811, 9839, 9843, 9874, 9879, 9892, 9898, 9918, 9931, 9935, 9937, 9939, 9940, 9979, 9987, 9991, 10003, 10022, 10023, 10024, 10025, 10026, 10061, 10066, 10072, 10077, 10083, 10084, 10086, 10090, 10091, 10102, 10116, 10119, 10120, 10143, 10149, 10151, 10153, 10195, 10198, 10234, 10237, 10266, 10269, 10289, 10300, 10315, 10328, 10329, 10338, 10356, 10364, 10373, 10375, 10386, 10400, 10402, 10406, 10432, 10474, 10492, 10499, 10500, 10514, 10531, 10546, 10645, 10651, 10737, 10759, 10770, 10774, 10883, 10903, 11068, 11069, 11072, 11076, 11079, 11085, 11091, 11093, 11094, 11095, 11096, 11097, 11114, 11118, 11119, 11121, 11125, 11134, 11140, 11141, 11144, 11149, 11150, 11151, 11152, 11154, 11162, 11165, 11172, 11174, 11177, 11180, 11181, 11183, 11184, 11185, 11186, 11187, 11188, 11189, 11191, 11192, 11194, 11198, 11199, 11201, 11202, 11203, 11213, 11214, 11215, 11217, 11221, 11223, 11226, 11229, 11231, 11232, 11233, 11236, 11237, 11238, 11239, 11242, 11247, 11248, 11255, 11257, 11260, 11265, 11266, 11270, 11271, 11272, 11273, 11275, 11276, 11283, 11285, 11289, 11290, 11293, 11296, 11300, 11303, 11310, 11313, 11314, 11315, 11318, 11321, 11329, 11330, 11332, 11333, 11334, 11335, 11336, 11342, 11345, 11358, 11364, 11367, 11371, 11372, 11374, 11377, 11379, 11380, 11383, 11384, 11385, 11389, 11392, 11395, 11396, 11397, 11401, 11402, 11407, 11411, 11413, 11414, 11415, 11417, 11418, 11419, 11420, 11421, 11427, 11440, 11442, 11444, 11445, 11452, 11456, 11460, 11463, 11464, 11466, 11470, 11474, 11476, 11480, 11481, 11487, 11493, 11495, 11499, 11500, 11501, 11507, 11508, 11509, 11510, 11520, 11522, 11523, 11525, 11531, 11533, 11534, 11535, 11536, 11537, 11542, 11549, 11550, 11552, 11553, 11557, 11559, 11560, 11561, 11563, 11564, 11565, 11568, 11569, 11570, 11573, 11574, 11575, 11577, 11579, 11582, 11583, 11586, 11590, 11594, 11599, 11600, 11601, 11604, 11605, 11607, 11609, 11616, 11620, 11627, 11632, 11635, 11637, 11643, 11650, 11651, 11652, 11654, 11657, 11660, 11661, 11669, 11671, 11675, 11677, 11684, 11685, 11688, 11693, 11698, 11700, 11701, 11706, 11708, 11715, 11717, 11719, 11720, 11722, 11723, 11728, 11731, 11737, 11738, 11740, 11745, 11752, 11753, 11754, 11761, 11762, 11769, 11770, 11776, 11778, 11788, 11790, 11791, 11792, 11794, 11796, 11798, 11799, 11803, 11805, 11806, 11818, 11819, 11828, 11829, 11830, 11832, 11833, 11835, 11836, 11838, 11839, 11844, 11845, 11850, 11857, 11858, 11859, 11862, 11864, 11867, 11868, 11872, 11875, 11876, 11884, 11888, 11889, 11892, 11893, 11896, 11909, 11910, 11912, 11913, 11917, 11919, 11920, 11922, 11926, 11927, 11930, 11931, 11936, 11939, 11941, 11948, 11949, 11951, 11955, 11956, 11958, 11960, 11961, 11963, 11964, 11965, 11966, 11970, 11971, 11972, 11973, 11976, 11978, 11979, 11980, 11981, 11984, 11985, 11987, 11989, 11990, 12002, 12004, 12005, 12010, 12012, 12015, 12016, 12018, 12019, 12022, 12023, 12028, 12030] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os.listdir(\u001b[33m\"\u001b[39m\u001b[33m../results\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_indices\u001b[49m\u001b[43m)\u001b[49m.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m         acc_no_na = df\n\u001b[32m      6\u001b[39m         new_row = pd.DataFrame({\n\u001b[32m      7\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m: [file.replace(\u001b[33m\"\u001b[39m\u001b[33mmmlu_pro_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m_eval_all_None.csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip().capitalize()],\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing answer\u001b[39m\u001b[33m\"\u001b[39m: [df[\u001b[33m\"\u001b[39m\u001b[33mextracted_answer\u001b[39m\u001b[33m\"\u001b[39m].isna().sum()],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mStd verbal linguistic confidence\u001b[39m\u001b[33m\"\u001b[39m: [df[\u001b[33m\"\u001b[39m\u001b[33mverbal_linguistic_confidence\u001b[39m\u001b[33m\"\u001b[39m].std()],\n\u001b[32m     26\u001b[39m         })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-uncertainty/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-uncertainty/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-uncertainty/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4828\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4830\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4831\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4833\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4834\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm-uncertainty/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: '[18, 35, 39, 51, 64, 68, 70, 74, 104, 137, 141, 157, 172, 199, 200, 249, 252, 267, 268, 331, 332, 338, 339, 371, 382, 395, 400, 405, 407, 416, 420, 422, 435, 451, 452, 485, 522, 540, 568, 579, 589, 590, 595, 602, 617, 628, 630, 701, 709, 724, 731, 738, 764, 783, 786, 797, 832, 851, 923, 1010, 1209, 1310, 1368, 1432, 1477, 1478, 1484, 1620, 1670, 1686, 1694, 1772, 1806, 1893, 1947, 2019, 2028, 2090, 2711, 2769, 2798, 2825, 2885, 2958, 2966, 3162, 3169, 3210, 3283, 3286, 3336, 3383, 3421, 3435, 3441, 3452, 3459, 3462, 3464, 3468, 3476, 3478, 3490, 3491, 3494, 3495, 3496, 3501, 3522, 3529, 3530, 3531, 3537, 3550, 3552, 3554, 3561, 3564, 3576, 3583, 3587, 3591, 3605, 3614, 3623, 3624, 3636, 3653, 3663, 3666, 3669, 3673, 3682, 3689, 3694, 3715, 3720, 3725, 3736, 3745, 3751, 3754, 3776, 3794, 3796, 3802, 3815, 3825, 3835, 3836, 3846, 3847, 3851, 3853, 3856, 3859, 3868, 3869, 3870, 3875, 3888, 3890, 3900, 3902, 3913, 3930, 3935, 3939, 3941, 3964, 3968, 3972, 3974, 3975, 3977, 3983, 3985, 3993, 3994, 3995, 3996, 4002, 4014, 4021, 4026, 4029, 4042, 4044, 4048, 4049, 4055, 4059, 4062, 4063, 4073, 4077, 4082, 4083, 4088, 4092, 4095, 4096, 4097, 4107, 4110, 4112, 4116, 4129, 4136, 4148, 4160, 4169, 4170, 4177, 4182, 4192, 4194, 4199, 4203, 4219, 4224, 4237, 4248, 4256, 4257, 4258, 4266, 4286, 4318, 4333, 4337, 4339, 4349, 4357, 4360, 4365, 4367, 4402, 4417, 4420, 4422, 4427, 4432, 4437, 4456, 4461, 4481, 4488, 4508, 4521, 4523, 4524, 4527, 4603, 4610, 4654, 4754, 4802, 4864, 5062, 5074, 5194, 5196, 5201, 5207, 5247, 5261, 5269, 5561, 5689, 5766, 5866, 5883, 5914, 6038, 6060, 6157, 6380, 6406, 6466, 6492, 6676, 6713, 6915, 6996, 7023, 7094, 7201, 7351, 7391, 7422, 7476, 7480, 7554, 7566, 7569, 7575, 7580, 7588, 7590, 7594, 7598, 7599, 7646, 7655, 7661, 7666, 7669, 7671, 7673, 7674, 7684, 7687, 7689, 7691, 7695, 7696, 7731, 7745, 7756, 7765, 7768, 7774, 7777, 7779, 7783, 7790, 7791, 7796, 7797, 7801, 7808, 7817, 7826, 7853, 7857, 7869, 7886, 7890, 7896, 7897, 7902, 7910, 7915, 7920, 7971, 7990, 7993, 8000, 8001, 8005, 8017, 8026, 8027, 8030, 8031, 8036, 8081, 8094, 8100, 8102, 8106, 8107, 8110, 8114, 8119, 8123, 8124, 8125, 8126, 8129, 8130, 8131, 8136, 8155, 8197, 8213, 8217, 8222, 8230, 8232, 8233, 8237, 8241, 8242, 8244, 8246, 8313, 8317, 8322, 8323, 8324, 8327, 8337, 8339, 8345, 8346, 8349, 8405, 8416, 8422, 8425, 8427, 8428, 8433, 8434, 8443, 8444, 8446, 8452, 8454, 8458, 8524, 8537, 8538, 8539, 8541, 8548, 8554, 8560, 8566, 8574, 8576, 8577, 8609, 8621, 8624, 8625, 8626, 8646, 8647, 8651, 8653, 8655, 8656, 8659, 8660, 8663, 8664, 8668, 8671, 8696, 8744, 8747, 8749, 8750, 8752, 8753, 8754, 8756, 8759, 8762, 8768, 8771, 8775, 8835, 8836, 8843, 8844, 8846, 8878, 8891, 8895, 8897, 8907, 8909, 8910, 8938, 8946, 8948, 8964, 8966, 8969, 8971, 8973, 8974, 8985, 8990, 8997, 8999, 9015, 9017, 9032, 9036, 9037, 9051, 9061, 9077, 9079, 9082, 9084, 9102, 9106, 9136, 9138, 9147, 9148, 9154, 9184, 9189, 9192, 9207, 9250, 9260, 9261, 9265, 9269, 9273, 9278, 9285, 9303, 9317, 9319, 9321, 9322, 9329, 9350, 9365, 9378, 9383, 9384, 9394, 9404, 9405, 9407, 9411, 9415, 9418, 9426, 9431, 9433, 9464, 9479, 9480, 9482, 9494, 9501, 9516, 9556, 9558, 9559, 9563, 9572, 9583, 9593, 9594, 9606, 9618, 9650, 9652, 9665, 9666, 9668, 9694, 9707, 9710, 9716, 9723, 9724, 9726, 9727, 9728, 9731, 9733, 9738, 9765, 9766, 9770, 9775, 9811, 9839, 9843, 9874, 9879, 9892, 9898, 9918, 9931, 9935, 9937, 9939, 9940, 9979, 9987, 9991, 10003, 10022, 10023, 10024, 10025, 10026, 10061, 10066, 10072, 10077, 10083, 10084, 10086, 10090, 10091, 10102, 10116, 10119, 10120, 10143, 10149, 10151, 10153, 10195, 10198, 10234, 10237, 10266, 10269, 10289, 10300, 10315, 10328, 10329, 10338, 10356, 10364, 10373, 10375, 10386, 10400, 10402, 10406, 10432, 10474, 10492, 10499, 10500, 10514, 10531, 10546, 10645, 10651, 10737, 10759, 10770, 10774, 10883, 10903, 11068, 11069, 11072, 11076, 11079, 11085, 11091, 11093, 11094, 11095, 11096, 11097, 11114, 11118, 11119, 11121, 11125, 11134, 11140, 11141, 11144, 11149, 11150, 11151, 11152, 11154, 11162, 11165, 11172, 11174, 11177, 11180, 11181, 11183, 11184, 11185, 11186, 11187, 11188, 11189, 11191, 11192, 11194, 11198, 11199, 11201, 11202, 11203, 11213, 11214, 11215, 11217, 11221, 11223, 11226, 11229, 11231, 11232, 11233, 11236, 11237, 11238, 11239, 11242, 11247, 11248, 11255, 11257, 11260, 11265, 11266, 11270, 11271, 11272, 11273, 11275, 11276, 11283, 11285, 11289, 11290, 11293, 11296, 11300, 11303, 11310, 11313, 11314, 11315, 11318, 11321, 11329, 11330, 11332, 11333, 11334, 11335, 11336, 11342, 11345, 11358, 11364, 11367, 11371, 11372, 11374, 11377, 11379, 11380, 11383, 11384, 11385, 11389, 11392, 11395, 11396, 11397, 11401, 11402, 11407, 11411, 11413, 11414, 11415, 11417, 11418, 11419, 11420, 11421, 11427, 11440, 11442, 11444, 11445, 11452, 11456, 11460, 11463, 11464, 11466, 11470, 11474, 11476, 11480, 11481, 11487, 11493, 11495, 11499, 11500, 11501, 11507, 11508, 11509, 11510, 11520, 11522, 11523, 11525, 11531, 11533, 11534, 11535, 11536, 11537, 11542, 11549, 11550, 11552, 11553, 11557, 11559, 11560, 11561, 11563, 11564, 11565, 11568, 11569, 11570, 11573, 11574, 11575, 11577, 11579, 11582, 11583, 11586, 11590, 11594, 11599, 11600, 11601, 11604, 11605, 11607, 11609, 11616, 11620, 11627, 11632, 11635, 11637, 11643, 11650, 11651, 11652, 11654, 11657, 11660, 11661, 11669, 11671, 11675, 11677, 11684, 11685, 11688, 11693, 11698, 11700, 11701, 11706, 11708, 11715, 11717, 11719, 11720, 11722, 11723, 11728, 11731, 11737, 11738, 11740, 11745, 11752, 11753, 11754, 11761, 11762, 11769, 11770, 11776, 11778, 11788, 11790, 11791, 11792, 11794, 11796, 11798, 11799, 11803, 11805, 11806, 11818, 11819, 11828, 11829, 11830, 11832, 11833, 11835, 11836, 11838, 11839, 11844, 11845, 11850, 11857, 11858, 11859, 11862, 11864, 11867, 11868, 11872, 11875, 11876, 11884, 11888, 11889, 11892, 11893, 11896, 11909, 11910, 11912, 11913, 11917, 11919, 11920, 11922, 11926, 11927, 11930, 11931, 11936, 11939, 11941, 11948, 11949, 11951, 11955, 11956, 11958, 11960, 11961, 11963, 11964, 11965, 11966, 11970, 11971, 11972, 11973, 11976, 11978, 11979, 11980, 11981, 11984, 11985, 11987, 11989, 11990, 12002, 12004, 12005, 12010, 12012, 12015, 12016, 12018, 12019, 12022, 12023, 12028, 12030] not found in axis'"
     ]
    }
   ],
   "source": [
    "stats_df = pd.DataFrame()\n",
    "for file in os.listdir(\"../results\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        df = pd.read_csv(\"../results/\" + file).drop(index=nan_indices).reset_index(drop=True)\n",
    "        acc_no_na = df\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Model\": [file.replace(\"mmlu_pro_\", \"\").replace(\"_eval_all_None.csv\", \"\").strip().capitalize()],\n",
    "            \"Missing answer\": [df[\"extracted_answer\"].isna().sum()],\n",
    "            \"Missing verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].isna().sum()],\n",
    "            \"Missing logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].isna().sum()],\n",
    "            \"Missing verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].isna().sum()],\n",
    "            \"Accuracy\": [(df[\"extracted_answer\"] == df[\"correct_answer\"]).mean()],\n",
    "            \"Accuracy without na\": [(acc_no_na[\"extracted_answer\"] == acc_no_na[\"correct_answer\"]).mean()],\n",
    "            \"ECE verbal numerical confidence\": [calculate_ece(df[\"verbal_numerical_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"ECE logit perplexity confidence\": [calculate_ece(df[\"logit_perplexity_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"ECE verbal linguistic confidence\": [calculate_ece(df[\"verbal_linguistic_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"AUROC verbal numerical confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]).values, df[\"verbal_numerical_confidence\"].fillna(0).values),\n",
    "            \"AUROC logit perplexity confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]), df[\"logit_perplexity_confidence\"].fillna(0).values),\n",
    "            \"AUROC verbal linguistic confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]), df[\"verbal_linguistic_confidence\"].fillna(0).values),\n",
    "            \"Mean verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].mean()],\n",
    "            \"Mean logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].mean()],\n",
    "            \"Mean verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].mean()],\n",
    "            \"Std verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].std()],\n",
    "            \"Std logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].std()],\n",
    "            \"Std verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].std()],\n",
    "        })\n",
    "        stats_df = pd.concat([stats_df, new_row], ignore_index=True)\n",
    "    \n",
    "display(stats_df.sort_values(by=\"Model\", ignore_index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
