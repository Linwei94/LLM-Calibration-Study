{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6101d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d13d219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ece(confidences, accuracies, n_bins=10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the expected calibration error (ECE) given a list of confidence scores (0-1) and accuracy scores (0 or 1).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"conf\": confidences, \"acc\": accuracies}).dropna()\n",
    "\n",
    "    confidences = torch.tensor(df[\"conf\"].tolist())\n",
    "    accuracies = torch.tensor(df[\"acc\"].tolist())\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = torch.zeros(1)\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Calculated |confidence - accuracy| in each bin\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86d3eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12032"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_indices = set()\n",
    "\n",
    "for file in os.listdir(\"../results\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        df = pd.read_csv(\"../results/\" + file)\n",
    "\n",
    "        # Find indices where either column has NaN\n",
    "        indices_with_nan = df[\n",
    "            df[\"extracted_answer\"].isna() | df[\"verbal_numerical_confidence\"].isna()\n",
    "        ].index\n",
    "\n",
    "        # Combine indices across all files\n",
    "        nan_indices.update(indices_with_nan)\n",
    "\n",
    "# Convert to sorted list if needed\n",
    "nan_indices = sorted(nan_indices)\n",
    "len(nan_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16202836",
   "metadata": {},
   "source": [
    "# Drop NA on an Individual Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1af877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/afs/intern/fangwenhan/miniconda3/envs/llm-uncertainty/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Missing answer</th>\n",
       "      <th>Missing verbal numerical confidence</th>\n",
       "      <th>Missing logit perplexity confidence</th>\n",
       "      <th>Missing verbal linguistic confidence</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy without na</th>\n",
       "      <th>ECE verbal numerical confidence</th>\n",
       "      <th>ECE logit perplexity confidence</th>\n",
       "      <th>ECE verbal linguistic confidence</th>\n",
       "      <th>AUROC verbal numerical confidence</th>\n",
       "      <th>AUROC logit perplexity confidence</th>\n",
       "      <th>Mean verbal numerical confidence</th>\n",
       "      <th>Mean logit perplexity confidence</th>\n",
       "      <th>Mean verbal linguistic confidence</th>\n",
       "      <th>Std verbal numerical confidence</th>\n",
       "      <th>Std logit perplexity confidence</th>\n",
       "      <th>Std verbal linguistic confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma-2-27b-it</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.531666</td>\n",
       "      <td>0.532995</td>\n",
       "      <td>0.379636</td>\n",
       "      <td>0.352674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.612014</td>\n",
       "      <td>0.523690</td>\n",
       "      <td>0.909806</td>\n",
       "      <td>0.884340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141204</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gemma-2b-it</td>\n",
       "      <td>1270</td>\n",
       "      <td>668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133062</td>\n",
       "      <td>0.148764</td>\n",
       "      <td>0.777028</td>\n",
       "      <td>0.761161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511432</td>\n",
       "      <td>0.528202</td>\n",
       "      <td>35.249272</td>\n",
       "      <td>0.894223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3406.079319</td>\n",
       "      <td>0.051987</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma-3-12b-it</td>\n",
       "      <td>12032</td>\n",
       "      <td>12032</td>\n",
       "      <td>12032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemma-3-1b-it</td>\n",
       "      <td>2473</td>\n",
       "      <td>1214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138049</td>\n",
       "      <td>0.173763</td>\n",
       "      <td>0.793448</td>\n",
       "      <td>0.743742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587881</td>\n",
       "      <td>0.445034</td>\n",
       "      <td>0.939211</td>\n",
       "      <td>0.881791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180571</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma-3-27b-it</td>\n",
       "      <td>12032</td>\n",
       "      <td>12032</td>\n",
       "      <td>12032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gemma-3-4b-it</td>\n",
       "      <td>12032</td>\n",
       "      <td>12032</td>\n",
       "      <td>12032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama-3.2-3b-instruct</td>\n",
       "      <td>214</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311420</td>\n",
       "      <td>0.317059</td>\n",
       "      <td>0.535822</td>\n",
       "      <td>0.483946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597997</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.848269</td>\n",
       "      <td>0.795365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173768</td>\n",
       "      <td>0.066231</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Meta-llama-3.1-405b-instruct-turbo_batch</td>\n",
       "      <td>114</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>466</td>\n",
       "      <td>0.715010</td>\n",
       "      <td>0.721849</td>\n",
       "      <td>0.164731</td>\n",
       "      <td>0.160528</td>\n",
       "      <td>0.218436</td>\n",
       "      <td>0.721472</td>\n",
       "      <td>0.608945</td>\n",
       "      <td>0.885023</td>\n",
       "      <td>0.875538</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.119135</td>\n",
       "      <td>0.044896</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mistral-7b-instruct-v0.1</td>\n",
       "      <td>2151</td>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214927</td>\n",
       "      <td>0.261714</td>\n",
       "      <td>0.700416</td>\n",
       "      <td>0.661195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544028</td>\n",
       "      <td>0.438903</td>\n",
       "      <td>0.926236</td>\n",
       "      <td>0.876122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201095</td>\n",
       "      <td>0.048418</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mistral-nemo-instruct-2407</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402094</td>\n",
       "      <td>0.404786</td>\n",
       "      <td>0.580863</td>\n",
       "      <td>0.449232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523301</td>\n",
       "      <td>0.509669</td>\n",
       "      <td>0.983325</td>\n",
       "      <td>0.851326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060344</td>\n",
       "      <td>0.049183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mixtral-8x7b-v0.1</td>\n",
       "      <td>1443</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092420</td>\n",
       "      <td>0.105015</td>\n",
       "      <td>0.008523</td>\n",
       "      <td>0.754768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532408</td>\n",
       "      <td>0.504758</td>\n",
       "      <td>0.015554</td>\n",
       "      <td>0.847188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121502</td>\n",
       "      <td>0.053246</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Qwen2.5-14b-instruct</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569814</td>\n",
       "      <td>0.570668</td>\n",
       "      <td>0.321295</td>\n",
       "      <td>0.288153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683158</td>\n",
       "      <td>0.613629</td>\n",
       "      <td>0.891267</td>\n",
       "      <td>0.857967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109634</td>\n",
       "      <td>0.054540</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Qwen2.5-32b-instruct</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.659242</td>\n",
       "      <td>0.662878</td>\n",
       "      <td>0.252854</td>\n",
       "      <td>0.203461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708256</td>\n",
       "      <td>0.665774</td>\n",
       "      <td>0.911538</td>\n",
       "      <td>0.862703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125643</td>\n",
       "      <td>0.056761</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Qwen2.5-3b-instruct</td>\n",
       "      <td>238</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.388298</td>\n",
       "      <td>0.396134</td>\n",
       "      <td>0.554058</td>\n",
       "      <td>0.444184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606145</td>\n",
       "      <td>0.564661</td>\n",
       "      <td>0.946666</td>\n",
       "      <td>0.832481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137152</td>\n",
       "      <td>0.073785</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Qwen2.5-7b-instruct</td>\n",
       "      <td>58</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.505402</td>\n",
       "      <td>0.507850</td>\n",
       "      <td>0.390313</td>\n",
       "      <td>0.366994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642940</td>\n",
       "      <td>0.611824</td>\n",
       "      <td>0.896391</td>\n",
       "      <td>0.872396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094847</td>\n",
       "      <td>0.054149</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qwen3-0.6b</td>\n",
       "      <td>2306</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200299</td>\n",
       "      <td>0.247789</td>\n",
       "      <td>0.691891</td>\n",
       "      <td>0.631562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546083</td>\n",
       "      <td>0.538223</td>\n",
       "      <td>0.885019</td>\n",
       "      <td>0.831861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264321</td>\n",
       "      <td>0.059801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Qwen3-0.6b-base</td>\n",
       "      <td>3329</td>\n",
       "      <td>2296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197224</td>\n",
       "      <td>0.272665</td>\n",
       "      <td>0.736976</td>\n",
       "      <td>0.640971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.488874</td>\n",
       "      <td>0.971261</td>\n",
       "      <td>0.838195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151414</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Qwen3-1.7b</td>\n",
       "      <td>264</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.390209</td>\n",
       "      <td>0.398963</td>\n",
       "      <td>0.491705</td>\n",
       "      <td>0.548449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598064</td>\n",
       "      <td>0.611030</td>\n",
       "      <td>0.875566</td>\n",
       "      <td>0.938659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241260</td>\n",
       "      <td>0.029157</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Qwen3-1.7b-think</td>\n",
       "      <td>231</td>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.560376</td>\n",
       "      <td>0.326778</td>\n",
       "      <td>0.363263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724315</td>\n",
       "      <td>0.711066</td>\n",
       "      <td>0.885424</td>\n",
       "      <td>0.912880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173317</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Qwen3-14b</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629322</td>\n",
       "      <td>0.631632</td>\n",
       "      <td>0.267041</td>\n",
       "      <td>0.294918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706230</td>\n",
       "      <td>0.697831</td>\n",
       "      <td>0.896331</td>\n",
       "      <td>0.924239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109715</td>\n",
       "      <td>0.035738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Qwen3-14b-awq</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613614</td>\n",
       "      <td>0.614533</td>\n",
       "      <td>0.273410</td>\n",
       "      <td>0.320212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701310</td>\n",
       "      <td>0.692682</td>\n",
       "      <td>0.886370</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115654</td>\n",
       "      <td>0.032453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Qwen3-14b-base</td>\n",
       "      <td>213</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.581699</td>\n",
       "      <td>0.592182</td>\n",
       "      <td>0.338958</td>\n",
       "      <td>0.282598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.632375</td>\n",
       "      <td>0.581740</td>\n",
       "      <td>0.924956</td>\n",
       "      <td>0.864297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074858</td>\n",
       "      <td>0.053334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Qwen3-14b-think</td>\n",
       "      <td>113</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756566</td>\n",
       "      <td>0.763739</td>\n",
       "      <td>0.129329</td>\n",
       "      <td>0.119671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786770</td>\n",
       "      <td>0.748882</td>\n",
       "      <td>0.894385</td>\n",
       "      <td>0.876237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120138</td>\n",
       "      <td>0.044260</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Qwen3-30b-a3b</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.659159</td>\n",
       "      <td>0.662462</td>\n",
       "      <td>0.265543</td>\n",
       "      <td>0.271912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.733663</td>\n",
       "      <td>0.728696</td>\n",
       "      <td>0.925850</td>\n",
       "      <td>0.931071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109089</td>\n",
       "      <td>0.034404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qwen3-30b-a3b-base</td>\n",
       "      <td>221</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.570396</td>\n",
       "      <td>0.581068</td>\n",
       "      <td>0.343866</td>\n",
       "      <td>0.308058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634385</td>\n",
       "      <td>0.496692</td>\n",
       "      <td>0.915859</td>\n",
       "      <td>0.878453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143940</td>\n",
       "      <td>0.072823</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Qwen3-30b-a3b-think</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.771775</td>\n",
       "      <td>0.773060</td>\n",
       "      <td>0.122607</td>\n",
       "      <td>0.153267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.811807</td>\n",
       "      <td>0.789704</td>\n",
       "      <td>0.895469</td>\n",
       "      <td>0.925043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120362</td>\n",
       "      <td>0.030727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Qwen3-32b</td>\n",
       "      <td>706</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656167</td>\n",
       "      <td>0.697069</td>\n",
       "      <td>0.220443</td>\n",
       "      <td>0.194539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732951</td>\n",
       "      <td>0.612886</td>\n",
       "      <td>0.877494</td>\n",
       "      <td>0.850706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228535</td>\n",
       "      <td>0.062412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Qwen3-32b-awq</td>\n",
       "      <td>1742</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608211</td>\n",
       "      <td>0.711176</td>\n",
       "      <td>0.202165</td>\n",
       "      <td>0.255826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.788187</td>\n",
       "      <td>0.440909</td>\n",
       "      <td>0.811791</td>\n",
       "      <td>0.864038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334036</td>\n",
       "      <td>0.070964</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Qwen3-32b-think</td>\n",
       "      <td>709</td>\n",
       "      <td>1183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.752161</td>\n",
       "      <td>0.799258</td>\n",
       "      <td>0.117224</td>\n",
       "      <td>0.119677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814658</td>\n",
       "      <td>0.520781</td>\n",
       "      <td>0.932580</td>\n",
       "      <td>0.871838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082868</td>\n",
       "      <td>0.055747</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Qwen3-4b</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.527178</td>\n",
       "      <td>0.529377</td>\n",
       "      <td>0.369697</td>\n",
       "      <td>0.420454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693612</td>\n",
       "      <td>0.652720</td>\n",
       "      <td>0.897489</td>\n",
       "      <td>0.947631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121436</td>\n",
       "      <td>0.025008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Qwen3-4b-base</td>\n",
       "      <td>273</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.513630</td>\n",
       "      <td>0.525555</td>\n",
       "      <td>0.401186</td>\n",
       "      <td>0.338382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.655269</td>\n",
       "      <td>0.554263</td>\n",
       "      <td>0.920027</td>\n",
       "      <td>0.852013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129665</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Qwen3-4b-think</td>\n",
       "      <td>309</td>\n",
       "      <td>709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.673870</td>\n",
       "      <td>0.691632</td>\n",
       "      <td>0.187993</td>\n",
       "      <td>0.225152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798604</td>\n",
       "      <td>0.680472</td>\n",
       "      <td>0.887896</td>\n",
       "      <td>0.899022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131461</td>\n",
       "      <td>0.042445</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Qwen3-8b</td>\n",
       "      <td>41</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.585106</td>\n",
       "      <td>0.587107</td>\n",
       "      <td>0.319753</td>\n",
       "      <td>0.364919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713786</td>\n",
       "      <td>0.567781</td>\n",
       "      <td>0.906043</td>\n",
       "      <td>0.950025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117109</td>\n",
       "      <td>0.031409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Qwen3-8b-base</td>\n",
       "      <td>2149</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456117</td>\n",
       "      <td>0.555297</td>\n",
       "      <td>0.300248</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767521</td>\n",
       "      <td>0.395210</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>0.883243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348216</td>\n",
       "      <td>0.062185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Qwen3-8b-think</td>\n",
       "      <td>168</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724069</td>\n",
       "      <td>0.734322</td>\n",
       "      <td>0.121647</td>\n",
       "      <td>0.166863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.811526</td>\n",
       "      <td>0.730404</td>\n",
       "      <td>0.858951</td>\n",
       "      <td>0.890932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145081</td>\n",
       "      <td>0.041105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Model  Missing answer  \\\n",
       "0                             Gemma-2-27b-it              30   \n",
       "1                                Gemma-2b-it            1270   \n",
       "2                             Gemma-3-12b-it           12032   \n",
       "3                              Gemma-3-1b-it            2473   \n",
       "4                             Gemma-3-27b-it           12032   \n",
       "5                              Gemma-3-4b-it           12032   \n",
       "6                      Llama-3.2-3b-instruct             214   \n",
       "7   Meta-llama-3.1-405b-instruct-turbo_batch             114   \n",
       "8                   Mistral-7b-instruct-v0.1            2151   \n",
       "9                 Mistral-nemo-instruct-2407              80   \n",
       "10                         Mixtral-8x7b-v0.1            1443   \n",
       "11                      Qwen2.5-14b-instruct              18   \n",
       "12                      Qwen2.5-32b-instruct              66   \n",
       "13                       Qwen2.5-3b-instruct             238   \n",
       "14                       Qwen2.5-7b-instruct              58   \n",
       "15                                Qwen3-0.6b            2306   \n",
       "16                           Qwen3-0.6b-base            3329   \n",
       "17                                Qwen3-1.7b             264   \n",
       "18                          Qwen3-1.7b-think             231   \n",
       "19                                 Qwen3-14b              44   \n",
       "20                             Qwen3-14b-awq              18   \n",
       "21                            Qwen3-14b-base             213   \n",
       "22                           Qwen3-14b-think             113   \n",
       "23                             Qwen3-30b-a3b              60   \n",
       "24                        Qwen3-30b-a3b-base             221   \n",
       "25                       Qwen3-30b-a3b-think              20   \n",
       "26                                 Qwen3-32b             706   \n",
       "27                             Qwen3-32b-awq            1742   \n",
       "28                           Qwen3-32b-think             709   \n",
       "29                                  Qwen3-4b              50   \n",
       "30                             Qwen3-4b-base             273   \n",
       "31                            Qwen3-4b-think             309   \n",
       "32                                  Qwen3-8b              41   \n",
       "33                             Qwen3-8b-base            2149   \n",
       "34                            Qwen3-8b-think             168   \n",
       "\n",
       "    Missing verbal numerical confidence  Missing logit perplexity confidence  \\\n",
       "0                                     0                                    0   \n",
       "1                                   668                                    0   \n",
       "2                                 12032                                12032   \n",
       "3                                  1214                                    0   \n",
       "4                                 12032                                12032   \n",
       "5                                 12032                                12032   \n",
       "6                                   196                                    0   \n",
       "7                                   111                                    0   \n",
       "8                                   883                                    0   \n",
       "9                                    11                                    0   \n",
       "10                                  228                                    0   \n",
       "11                                   10                                    0   \n",
       "12                                   15                                    0   \n",
       "13                                  180                                    0   \n",
       "14                                   22                                    0   \n",
       "15                                   21                                    0   \n",
       "16                                 2296                                    0   \n",
       "17                                   55                                    0   \n",
       "18                                  345                                    0   \n",
       "19                                   14                                    0   \n",
       "20                                   10                                    0   \n",
       "21                                  105                                    0   \n",
       "22                                  170                                    0   \n",
       "23                                   30                                    0   \n",
       "24                                   57                                    0   \n",
       "25                                   27                                    0   \n",
       "26                                   27                                    0   \n",
       "27                                   54                                    0   \n",
       "28                                 1183                                    0   \n",
       "29                                   14                                    0   \n",
       "30                                  127                                    0   \n",
       "31                                  709                                    0   \n",
       "32                                   26                                    0   \n",
       "33                                   95                                    0   \n",
       "34                                  312                                    0   \n",
       "\n",
       "    Missing verbal linguistic confidence  Accuracy  Accuracy without na  \\\n",
       "0                                      0  0.531666             0.532995   \n",
       "1                                      0  0.133062             0.148764   \n",
       "2                                      0  0.000000                  NaN   \n",
       "3                                      0  0.138049             0.173763   \n",
       "4                                      0  0.000000                  NaN   \n",
       "5                                      0  0.000000                  NaN   \n",
       "6                                      0  0.311420             0.317059   \n",
       "7                                    466  0.715010             0.721849   \n",
       "8                                      0  0.214927             0.261714   \n",
       "9                                      0  0.402094             0.404786   \n",
       "10                                     0  0.092420             0.105015   \n",
       "11                                     0  0.569814             0.570668   \n",
       "12                                     0  0.659242             0.662878   \n",
       "13                                     0  0.388298             0.396134   \n",
       "14                                     0  0.505402             0.507850   \n",
       "15                                     0  0.200299             0.247789   \n",
       "16                                     0  0.197224             0.272665   \n",
       "17                                     0  0.390209             0.398963   \n",
       "18                                     0  0.549618             0.560376   \n",
       "19                                     0  0.629322             0.631632   \n",
       "20                                     0  0.613614             0.614533   \n",
       "21                                     0  0.581699             0.592182   \n",
       "22                                     0  0.756566             0.763739   \n",
       "23                                     0  0.659159             0.662462   \n",
       "24                                     0  0.570396             0.581068   \n",
       "25                                     0  0.771775             0.773060   \n",
       "26                                     0  0.656167             0.697069   \n",
       "27                                     0  0.608211             0.711176   \n",
       "28                                     0  0.752161             0.799258   \n",
       "29                                     0  0.527178             0.529377   \n",
       "30                                     0  0.513630             0.525555   \n",
       "31                                     0  0.673870             0.691632   \n",
       "32                                     0  0.585106             0.587107   \n",
       "33                                     0  0.456117             0.555297   \n",
       "34                                     0  0.724069             0.734322   \n",
       "\n",
       "    ECE verbal numerical confidence  ECE logit perplexity confidence  \\\n",
       "0                          0.379636                         0.352674   \n",
       "1                          0.777028                         0.761161   \n",
       "2                          0.000000                         0.000000   \n",
       "3                          0.793448                         0.743742   \n",
       "4                          0.000000                         0.000000   \n",
       "5                          0.000000                         0.000000   \n",
       "6                          0.535822                         0.483946   \n",
       "7                          0.164731                         0.160528   \n",
       "8                          0.700416                         0.661195   \n",
       "9                          0.580863                         0.449232   \n",
       "10                         0.008523                         0.754768   \n",
       "11                         0.321295                         0.288153   \n",
       "12                         0.252854                         0.203461   \n",
       "13                         0.554058                         0.444184   \n",
       "14                         0.390313                         0.366994   \n",
       "15                         0.691891                         0.631562   \n",
       "16                         0.736976                         0.640971   \n",
       "17                         0.491705                         0.548449   \n",
       "18                         0.326778                         0.363263   \n",
       "19                         0.267041                         0.294918   \n",
       "20                         0.273410                         0.320212   \n",
       "21                         0.338958                         0.282598   \n",
       "22                         0.129329                         0.119671   \n",
       "23                         0.265543                         0.271912   \n",
       "24                         0.343866                         0.308058   \n",
       "25                         0.122607                         0.153267   \n",
       "26                         0.220443                         0.194539   \n",
       "27                         0.202165                         0.255826   \n",
       "28                         0.117224                         0.119677   \n",
       "29                         0.369697                         0.420454   \n",
       "30                         0.401186                         0.338382   \n",
       "31                         0.187993                         0.225152   \n",
       "32                         0.319753                         0.364919   \n",
       "33                         0.300248                         0.427126   \n",
       "34                         0.121647                         0.166863   \n",
       "\n",
       "    ECE verbal linguistic confidence  AUROC verbal numerical confidence  \\\n",
       "0                           0.000000                           0.612014   \n",
       "1                           0.000000                           0.511432   \n",
       "2                           0.000000                                NaN   \n",
       "3                           0.000000                           0.587881   \n",
       "4                           0.000000                                NaN   \n",
       "5                           0.000000                                NaN   \n",
       "6                           0.000000                           0.597997   \n",
       "7                           0.218436                           0.721472   \n",
       "8                           0.000000                           0.544028   \n",
       "9                           0.000000                           0.523301   \n",
       "10                          0.000000                           0.532408   \n",
       "11                          0.000000                           0.683158   \n",
       "12                          0.000000                           0.708256   \n",
       "13                          0.000000                           0.606145   \n",
       "14                          0.000000                           0.642940   \n",
       "15                          0.000000                           0.546083   \n",
       "16                          0.000000                           0.605870   \n",
       "17                          0.000000                           0.598064   \n",
       "18                          0.000000                           0.724315   \n",
       "19                          0.000000                           0.706230   \n",
       "20                          0.000000                           0.701310   \n",
       "21                          0.000000                           0.632375   \n",
       "22                          0.000000                           0.786770   \n",
       "23                          0.000000                           0.733663   \n",
       "24                          0.000000                           0.634385   \n",
       "25                          0.000000                           0.811807   \n",
       "26                          0.000000                           0.732951   \n",
       "27                          0.000000                           0.788187   \n",
       "28                          0.000000                           0.814658   \n",
       "29                          0.000000                           0.693612   \n",
       "30                          0.000000                           0.655269   \n",
       "31                          0.000000                           0.798604   \n",
       "32                          0.000000                           0.713786   \n",
       "33                          0.000000                           0.767521   \n",
       "34                          0.000000                           0.811526   \n",
       "\n",
       "    AUROC logit perplexity confidence  Mean verbal numerical confidence  \\\n",
       "0                            0.523690                          0.909806   \n",
       "1                            0.528202                         35.249272   \n",
       "2                                 NaN                               NaN   \n",
       "3                            0.445034                          0.939211   \n",
       "4                                 NaN                               NaN   \n",
       "5                                 NaN                               NaN   \n",
       "6                            0.509987                          0.848269   \n",
       "7                            0.608945                          0.885023   \n",
       "8                            0.438903                          0.926236   \n",
       "9                            0.509669                          0.983325   \n",
       "10                           0.504758                          0.015554   \n",
       "11                           0.613629                          0.891267   \n",
       "12                           0.665774                          0.911538   \n",
       "13                           0.564661                          0.946666   \n",
       "14                           0.611824                          0.896391   \n",
       "15                           0.538223                          0.885019   \n",
       "16                           0.488874                          0.971261   \n",
       "17                           0.611030                          0.875566   \n",
       "18                           0.711066                          0.885424   \n",
       "19                           0.697831                          0.896331   \n",
       "20                           0.692682                          0.886370   \n",
       "21                           0.581740                          0.924956   \n",
       "22                           0.748882                          0.894385   \n",
       "23                           0.728696                          0.925850   \n",
       "24                           0.496692                          0.915859   \n",
       "25                           0.789704                          0.895469   \n",
       "26                           0.612886                          0.877494   \n",
       "27                           0.440909                          0.811791   \n",
       "28                           0.520781                          0.932580   \n",
       "29                           0.652720                          0.897489   \n",
       "30                           0.554263                          0.920027   \n",
       "31                           0.680472                          0.887896   \n",
       "32                           0.567781                          0.906043   \n",
       "33                           0.395210                          0.759224   \n",
       "34                           0.730404                          0.858951   \n",
       "\n",
       "    Mean logit perplexity confidence  Mean verbal linguistic confidence  \\\n",
       "0                           0.884340                                0.0   \n",
       "1                           0.894223                                0.0   \n",
       "2                                NaN                                0.0   \n",
       "3                           0.881791                                0.0   \n",
       "4                                NaN                                0.0   \n",
       "5                                NaN                                0.0   \n",
       "6                           0.795365                                0.0   \n",
       "7                           0.875538                                inf   \n",
       "8                           0.876122                                0.0   \n",
       "9                           0.851326                                0.0   \n",
       "10                          0.847188                                0.0   \n",
       "11                          0.857967                                0.0   \n",
       "12                          0.862703                                0.0   \n",
       "13                          0.832481                                0.0   \n",
       "14                          0.872396                                0.0   \n",
       "15                          0.831861                                0.0   \n",
       "16                          0.838195                                0.0   \n",
       "17                          0.938659                                0.0   \n",
       "18                          0.912880                                0.0   \n",
       "19                          0.924239                                0.0   \n",
       "20                          0.933826                                0.0   \n",
       "21                          0.864297                                0.0   \n",
       "22                          0.876237                                0.0   \n",
       "23                          0.931071                                0.0   \n",
       "24                          0.878453                                0.0   \n",
       "25                          0.925043                                0.0   \n",
       "26                          0.850706                                0.0   \n",
       "27                          0.864038                                0.0   \n",
       "28                          0.871838                                0.0   \n",
       "29                          0.947631                                0.0   \n",
       "30                          0.852013                                0.0   \n",
       "31                          0.899022                                0.0   \n",
       "32                          0.950025                                0.0   \n",
       "33                          0.883243                                0.0   \n",
       "34                          0.890932                                0.0   \n",
       "\n",
       "    Std verbal numerical confidence  Std logit perplexity confidence  \\\n",
       "0                          0.141204                         0.046461   \n",
       "1                       3406.079319                         0.051987   \n",
       "2                               NaN                              NaN   \n",
       "3                          0.180571                         0.056061   \n",
       "4                               NaN                              NaN   \n",
       "5                               NaN                              NaN   \n",
       "6                          0.173768                         0.066231   \n",
       "7                          0.119135                         0.044896   \n",
       "8                          0.201095                         0.048418   \n",
       "9                          0.060344                         0.049183   \n",
       "10                         0.121502                         0.053246   \n",
       "11                         0.109634                         0.054540   \n",
       "12                         0.125643                         0.056761   \n",
       "13                         0.137152                         0.073785   \n",
       "14                         0.094847                         0.054149   \n",
       "15                         0.264321                         0.059801   \n",
       "16                         0.151414                         0.081134   \n",
       "17                         0.241260                         0.029157   \n",
       "18                         0.173317                         0.028400   \n",
       "19                         0.109715                         0.035738   \n",
       "20                         0.115654                         0.032453   \n",
       "21                         0.074858                         0.053334   \n",
       "22                         0.120138                         0.044260   \n",
       "23                         0.109089                         0.034404   \n",
       "24                         0.143940                         0.072823   \n",
       "25                         0.120362                         0.030727   \n",
       "26                         0.228535                         0.062412   \n",
       "27                         0.334036                         0.070964   \n",
       "28                         0.082868                         0.055747   \n",
       "29                         0.121436                         0.025008   \n",
       "30                         0.129665                         0.064456   \n",
       "31                         0.131461                         0.042445   \n",
       "32                         0.117109                         0.031409   \n",
       "33                         0.348216                         0.062185   \n",
       "34                         0.145081                         0.041105   \n",
       "\n",
       "    Std verbal linguistic confidence  \n",
       "0                                0.0  \n",
       "1                                0.0  \n",
       "2                                0.0  \n",
       "3                                0.0  \n",
       "4                                0.0  \n",
       "5                                0.0  \n",
       "6                                0.0  \n",
       "7                                NaN  \n",
       "8                                0.0  \n",
       "9                                0.0  \n",
       "10                               0.0  \n",
       "11                               0.0  \n",
       "12                               0.0  \n",
       "13                               0.0  \n",
       "14                               0.0  \n",
       "15                               0.0  \n",
       "16                               0.0  \n",
       "17                               0.0  \n",
       "18                               0.0  \n",
       "19                               0.0  \n",
       "20                               0.0  \n",
       "21                               0.0  \n",
       "22                               0.0  \n",
       "23                               0.0  \n",
       "24                               0.0  \n",
       "25                               0.0  \n",
       "26                               0.0  \n",
       "27                               0.0  \n",
       "28                               0.0  \n",
       "29                               0.0  \n",
       "30                               0.0  \n",
       "31                               0.0  \n",
       "32                               0.0  \n",
       "33                               0.0  \n",
       "34                               0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_df = pd.DataFrame()\n",
    "for file in os.listdir(\"../results\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        df = pd.read_csv(\"../results/\" + file)\n",
    "        acc_no_na = df[[\"extracted_answer\", \"correct_answer\"]].dropna()\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Model\": [file.replace(\"mmlu_pro_\", \"\").replace(\"_eval_all_None.csv\", \"\").strip().capitalize()],\n",
    "            \"Missing answer\": [df[\"extracted_answer\"].isna().sum()],\n",
    "            \"Missing verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].isna().sum()],\n",
    "            \"Missing logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].isna().sum()],\n",
    "            \"Missing verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].isna().sum()],\n",
    "            \"Accuracy\": [(df[\"extracted_answer\"] == df[\"correct_answer\"]).mean()],\n",
    "            \"Accuracy without na\": [(acc_no_na[\"extracted_answer\"] == acc_no_na[\"correct_answer\"]).mean()],\n",
    "            \"ECE verbal numerical confidence\": [calculate_ece(df[\"verbal_numerical_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"ECE logit perplexity confidence\": [calculate_ece(df[\"logit_perplexity_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"ECE verbal linguistic confidence\": [calculate_ece(df[\"verbal_linguistic_confidence\"].values, (df[\"extracted_answer\"] == df[\"correct_answer\"]))],\n",
    "            \"AUROC verbal numerical confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]).values, df[\"verbal_numerical_confidence\"].fillna(0).values),\n",
    "            \"AUROC logit perplexity confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]), df[\"logit_perplexity_confidence\"].fillna(0).values),\n",
    "            \"AUROC verbal linguistic confidence\": roc_auc_score((df[\"extracted_answer\"] == df[\"correct_answer\"]), df[\"verbal_linguistic_confidence\"].fillna(0).values),\n",
    "            \"Mean verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].mean()],\n",
    "            \"Mean logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].mean()],\n",
    "            \"Mean verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].mean()],\n",
    "            \"Std verbal numerical confidence\": [df[\"verbal_numerical_confidence\"].std()],\n",
    "            \"Std logit perplexity confidence\": [df[\"logit_perplexity_confidence\"].std()],\n",
    "            \"Std verbal linguistic confidence\": [df[\"verbal_linguistic_confidence\"].std()],\n",
    "        })\n",
    "        stats_df = pd.concat([stats_df, new_row], ignore_index=True)\n",
    "\n",
    "    \n",
    "display(stats_df.sort_values(by=\"Model\", ignore_index=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
